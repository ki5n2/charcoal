{
  
    
        "post0": {
            "title": "DS 6. 최적화문제, tf.keras.optimizers를 이용한 최적화방법, 회귀분석",
            "content": ". Data Science . lenture: Data Science_5-1nd week of lectures. | lenture date: 2022-03-30 | lecturer: Guebin choi | study date: 2022-03-30 | author: Kione kim | . . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$ . - 기존의 학습 $$ beta_{next} leftarrow beta_{old} - alpha left[ frac{ partial}{ partial beta} loss( beta) right]_{ beta= beta_{old}}$$ . beta = tf.Variable(-10.0) alpha=0.01/6 for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta . - 이 방식의 단점: . 위의 식을 알고 있어야 함 | 사람들 생각: (위의 식을 알지 정확하게 알지 못하더라도) $β_{old}$ 에서의 미분값과 $β_{old}$(현재 위치)를 통해 $β_{new}$(좀 더 나은 $β$)가 나오는 정도만 알고 있으면, 이미 구현되어 있는 식(위의 식)에 값을 넣기만 하면 계산되는 프로세스를 만들 수 있지 않을까? | 그래서 만든 것: optimizer | . tf.keras.optimizers&#47484; &#51060;&#50857;&#54620; &#52572;&#51201;&#54868;&#48169;&#48277; . &#48169;&#48277;1: opt.apply_gradients()&#47484; &#51060;&#50857; . - optimizer 정의 : 학습율 함께 정의 . alpha=0.01/6 . opt=tf.keras.optimizers.SGD(learning_rate=alpha) . . opt.learning_rate . &lt;tf.Variable &#39;learning_rate:0&#39; shape=() dtype=float32, numpy=0.0016666667&gt; . opt.lr . &lt;tf.Variable &#39;learning_rate:0&#39; shape=() dtype=float32, numpy=0.0016666667&gt; . id(opt.learning_rate) . 2567521054336 . id(opt.lr) . 2567521054336 . opt.learning_rate와 opt.lr은 같은 것임, 둘 중 쓰고 싶은 것 쓰면 됨! | . . - optimizer에 들어갈 입력값 정리 . beta | beta에서의 미분값 | . beta= tf.Variable(-10.0) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-10.0&gt; . with tf.GradientTape(persistent=True) as mytape: loss=(beta/2 -1)**2 slope=mytape.gradient(loss,beta) # 기울기라는 변수로 저장 slope . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . - .apply_gradients() . opt.apply_gradients([(slope, beta)]) # 튜플의 리스트화하여 입력 . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . beta값이 바꼈음 | 이 값은 -10에서 0.01만큼 움직인 값임! -&gt; 베타를 한 번 업데이트한 값 | 즉, 위 과정은 iter1과 같음 | 여기서 주의점: opt.apply_gradients()의 입력으로 pair의 list를 전달해야한다는 것 | . - iter2 . with tf.GradientTape(persistent=True) as mytape: loss=(beta/2 -1)**2 slope=mytape.gradient(loss,beta) . opt.apply_gradients([(slope, beta)]) # 튜플의 리스트화하여 입력 . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=2&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . 한 번 더 업데이트 되었음 | . - iter3:for문을 통한 반복 . alpha=0.01/6 opt=tf.keras.optimizers.SGD(alpha) beta=tf.Variable(-10.0) . for epoc in range(10000): with tf.GradientTape() as mytape: loss=(beta/2 -1)**2 slope=mytape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . 그런데 이게 더 어려운 것 아닌가..? | 차라리 식을 아는 게 더 편한 것 같음 | . 하지만, 외워야 할 식이 하나만 있는 것이 아니다..! 여러가지 최적화 방법들이 존재.. 각각의 방법들이 식이 조금씩 다르다 | 따라서 옵티마이저를 쓰는 방법을 알고 있으면 식을 알지 못하더라도 계산할 수 있다 | 결국은 옵티마이저를 쓰는 게 편리하다! | . &#48169;&#48277;2: opt.minimize()&#47484; &#51060;&#50857; . opt.apply_gradients()는 loss식 &amp; beta(old)값 &amp; beta(old)에서의 기울기(slope)를 전달해주면 되는 반면 opt.minimize()는 loss식 &amp; loss식을 무엇으로 미분할지만 전달해주면 알아서 계산해준다. . - α, β, optimizer 선언 . alpha=0.01/6 opt=tf.keras.optimizers.SGD(alpha) beta=tf.Variable(-10.0) . opt.minimize? . Signature: opt.minimize(loss, var_list, grad_loss=None, name=None, tape=None) Docstring: Minimize `loss` by updating `var_list`. This method simply computes gradient using `tf.GradientTape` and calls `apply_gradients()`. If you want to process the gradient before applying then call `tf.GradientTape` and `apply_gradients()` explicitly instead of using this function. Args: loss: `Tensor` or callable. If a callable, `loss` should take no arguments and return the value to minimize. If a `Tensor`, the `tape` argument must be passed. var_list: list or tuple of `Variable` objects to update to minimize `loss`, or a callable returning the list or tuple of `Variable` objects. Use callable when the variable list would otherwise be incomplete before `minimize` since the variables are created at the first time `loss` is called. grad_loss: (Optional). A `Tensor` holding the gradient computed for `loss`. name: (Optional) str. Name for the returned operation. tape: (Optional) `tf.GradientTape`. If `loss` is provided as a `Tensor`, the tape that computed the `loss` must be provided. Returns: An `Operation` that updates the variables in `var_list`. The `iterations` will be automatically increased by 1. Raises: ValueError: If some of the variables are not `Variable` objects. File: c: users kko anaconda3 envs ds2022 lib site-packages keras optimizer_v2 optimizer_v2.py Type: method . loss를 함수의 형태로 전달해야 함! | . loss_fn= lambda: (beta/2-1)**2 . opt.minimize(loss_fn,beta) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . numpy=1은 1번 업데이트 했다는 의미 | . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . $β$값을 보니 $-10.0$에서 한 번 업데이트한 값인 $-9.99$이 나타났음 | 즉, 위 과정은 iter1과 같음 | . - iter2:for문을 통한 반복 . alpha=0.01/6 opt=tf.keras.optimizers.SGD(alpha) beta=tf.Variable(-10.0) loss_fn= lambda: (beta/2-1)**2 . for epoc in range(10000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . 2에 근접한 값이 나왔음 | . loss_fn . &lt;function __main__.&lt;lambda&gt;()&gt; . loss_fn() . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0661923e-06&gt; . 위 값은 현재시점 β에 해당하는 loss값이다. | 다시 말해, loss_fn()을 실행하면, 현재시점에서의 $(beta/2-1)**2$ 값이 출력된다. | . &#44592;&#53440; . - tf.keras.optimizers.SGD와 tf.optimizers.SGD의 차이점 . _opt1=tf.keras.optimizers.SGD() _opt2=tf.optimizers.SGD() . type(_opt1), type(_opt2) . (keras.optimizer_v2.gradient_descent.SGD, keras.optimizer_v2.gradient_descent.SGD) . type이 같음! | . - tf.optimizers.SGD로 위의 과정 계산 . alpha=0.01/6 opt=tf.optimizers.SGD(alpha) beta=tf.Variable(-10.0) loss_fn= lambda: (beta/2-1)**2 . for epoc in range(10000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . 똑같은 값이 나옴 -&gt; 같은 기능 | . - 다른 방법 . tf.optimizers? . Type: module String form: &lt;module &#39;keras.api._v2.keras.optimizers&#39; from &#39;C: Users kko anaconda3 envs ds2022 lib site-packages keras api _v2 keras optimizers __init__.py&#39;&gt; File: c: users kko anaconda3 envs ds2022 lib site-packages keras api _v2 keras optimizers __init__.py Docstring: Public API for tf.keras.optimizers namespace. . tf.keras.optimizers? . Type: module String form: &lt;module &#39;keras.api._v2.keras.optimizers&#39; from &#39;C: Users kko anaconda3 envs ds2022 lib site-packages keras api _v2 keras optimizers __init__.py&#39;&gt; File: c: users kko anaconda3 envs ds2022 lib site-packages keras api _v2 keras optimizers __init__.py Docstring: Public API for tf.keras.optimizers namespace. . 파일이 위치한 장소가 같음! | . 차이 없음! | . &#54924;&#44480;&#48516;&#49437; . - ${ bf y} approx 4 + 2.5 { bf x}$에서 $4$와 $2.5$를 모른다고 생각하고 $β_{0}, β_{1}$를 구하는 방법 . - 선언 . tnp.random.seed(50000) n= 500 # samplesize x= tnp.linspace(0,1,n) epsilon= tnp.random.randn(n)*0.5 . y=4+2.5*x+epsilon y_true=4+2.5*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x255cc386430&gt;] . 파란색 점들을 통해 빨강색 점을 추론 | . &#54400;&#51060;1 : $ beta$&#44396;&#54616;&#45716; &#44277;&#49885;&#51012; &#53685;&#54620; &#54400;&#51060; . tnp.random.seed(50000) n= 500 x= tnp.linspace(0,1,n) epsilon= tnp.random.randn(n)*0.5 y=4+2.5*x+epsilon . Sxx= sum((x-np.mean(x))**2) Sxy= sum((x-np.mean(x))*(y-np.mean(y))) . beta1_h = Sxy/Sxx beta0_h = np.mean(y) - beta1_h*np.mean(x) . beta0_h,beta1_h . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=4.043123346730244&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.4858994002326082&gt;) . 거의 근접한 값이 나옴 | . &#54400;&#51060;2 : &#48289;&#53552; &amp; &#47588;&#53944;&#47533;&#49828;&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; . tnp.random.seed(50000) n= 500 x= tnp.linspace(0,1,n) epsilon= tnp.random.randn(n)*0.5 y=4+2.5*x+epsilon . - X선언 . x[:20] . &lt;tf.Tensor: shape=(20,), dtype=float64, numpy= array([0. , 0.00200401, 0.00400802, 0.00601202, 0.00801603, 0.01002004, 0.01202405, 0.01402806, 0.01603206, 0.01803607, 0.02004008, 0.02204409, 0.0240481 , 0.0260521 , 0.02805611, 0.03006012, 0.03206413, 0.03406814, 0.03607214, 0.03807615])&gt; . tf.ones(n)[:20] . &lt;tf.Tensor: shape=(20,), dtype=float32, numpy= array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)&gt; . X=tf.stack([tf.ones(n,dtype=&#39;float64&#39;),x],axis=1) X[:20] . &lt;tf.Tensor: shape=(20, 2), dtype=float64, numpy= array([[1. , 0. ], [1. , 0.00200401], [1. , 0.00400802], [1. , 0.00601202], [1. , 0.00801603], [1. , 0.01002004], [1. , 0.01202405], [1. , 0.01402806], [1. , 0.01603206], [1. , 0.01803607], [1. , 0.02004008], [1. , 0.02204409], [1. , 0.0240481 ], [1. , 0.0260521 ], [1. , 0.02805611], [1. , 0.03006012], [1. , 0.03206413], [1. , 0.03406814], [1. , 0.03607214], [1. , 0.03807615]])&gt; . y[:20] . &lt;tf.Tensor: shape=(20,), dtype=float64, numpy= array([3.35275314, 4.22611941, 4.87242315, 5.31953133, 3.8417442 , 2.7444039 , 3.75847533, 3.67882462, 3.84180216, 3.54850093, 4.77493479, 4.64718767, 4.2011182 , 4.31115921, 4.81145538, 2.94308534, 4.03927822, 4.3866718 , 3.96485239, 3.96641934])&gt; . y=y.reshape(n,1) . y[:20] . &lt;tf.Tensor: shape=(20, 1), dtype=float64, numpy= array([[3.35275314], [4.22611941], [4.87242315], [5.31953133], [3.8417442 ], [2.7444039 ], [3.75847533], [3.67882462], [3.84180216], [3.54850093], [4.77493479], [4.64718767], [4.2011182 ], [4.31115921], [4.81145538], [2.94308534], [4.03927822], [4.3866718 ], [3.96485239], [3.96641934]])&gt; . X.shape,Y.shape . (TensorShape([500, 2]), TensorShape([500, 1])) . tf.linalg.inv(X.T@X) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[4.04312335], [2.4858994 ]])&gt; . &#54400;&#51060;3: (&#53584;&#49436;&#54540;&#47196;&#50864;&#47484; &#50416;&#51648; &#50506;&#44256;) &#44221;&#49324;&#54616;&#44053;&#48277;&#51012; &#51060;&#50857;&#54620; &#54400;&#51060; . : 도함수를 알고 있기 때문 . tnp.random.seed(50000) n= 500 x= tnp.linspace(0,1,n) epsilon= tnp.random.randn(n)*0.5 y=4+2.5*x+epsilon . X=tf.stack([tf.ones(n,dtype=&#39;float64&#39;),x],axis=1) X[:20] . &lt;tf.Tensor: shape=(20, 2), dtype=float64, numpy= array([[1. , 0. ], [1. , 0.00200401], [1. , 0.00400802], [1. , 0.00601202], [1. , 0.00801603], [1. , 0.01002004], [1. , 0.01202405], [1. , 0.01402806], [1. , 0.01603206], [1. , 0.01803607], [1. , 0.02004008], [1. , 0.02204409], [1. , 0.0240481 ], [1. , 0.0260521 ], [1. , 0.02805611], [1. , 0.03006012], [1. , 0.03206413], [1. , 0.03406814], [1. , 0.03607214], [1. , 0.03807615]])&gt; . y=y.reshape(n,1) y[:20] . &lt;tf.Tensor: shape=(20, 1), dtype=float64, numpy= array([[3.35275314], [4.22611941], [4.87242315], [5.31953133], [3.8417442 ], [2.7444039 ], [3.75847533], [3.67882462], [3.84180216], [3.54850093], [4.77493479], [4.64718767], [4.2011182 ], [4.31115921], [4.81145538], [2.94308534], [4.03927822], [4.3866718 ], [3.96485239], [3.96641934]])&gt; . X.shape, y.shape . (TensorShape([500, 2]), TensorShape([500, 1])) . - 임의의 beta 선언 . beta= tnp.array([-5.0,-2.0]).reshape(2,1) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-5.], [-2.]])&gt; . - 미분계수를 통한 slope선언 (loss&#39; = slope) . cf. $loss = (y-yhat)&#39;(y-yhat) =&gt; (y-x*beta)&#39;(y-x*beta) =&gt; loss&#39;= -2x&#39;y +2x&#39;xb$ . slope=-2*X.T @ y + 2*X.T @ X @beta slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-11286.07304685], [ -6018.35976984]])&gt; . $$ beta_{next} leftarrow beta_{old} - alpha left[ frac{ partial}{ partial beta} loss( beta) right]_{ beta= beta_{old}}$$ : $step = alpha * loss&#39;$, $loss&#39; = slope$ -&gt; $step$= $alpha * slope$ . step = -alpha * slope step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[18.81012174], [10.03059962]])&gt; . - 반복 . for epoc in range(1000): slope= -2*X.T @ y + 2*X.T @ X @ beta step = -slope * alpha beta = beta + step . beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[4.04312335], [2.4858994 ]])&gt; . &#52628;&#44032;&#54617;&#49845; . 풀이3을 완성하라. 즉 경사하강법을 이용하여 적절한 beta를 추정하라. . iteration 횟수는 1000번으로 설정 | 학습률은 0.001로 설정 | beta의 초기값은 beta= tnp.array([-5.0,10.0]).reshape(2,1) | . tnp.random.seed(50000) n= 500 x= tnp.linspace(0,1,n) epsilon= tnp.random.randn(n)*0.5 y=4+2.5*x+epsilon . X=tf.stack([tf.ones(n,dtype=&#39;float64&#39;),x],axis=1) X[:20] . &lt;tf.Tensor: shape=(20, 2), dtype=float64, numpy= array([[1. , 0. ], [1. , 0.00200401], [1. , 0.00400802], [1. , 0.00601202], [1. , 0.00801603], [1. , 0.01002004], [1. , 0.01202405], [1. , 0.01402806], [1. , 0.01603206], [1. , 0.01803607], [1. , 0.02004008], [1. , 0.02204409], [1. , 0.0240481 ], [1. , 0.0260521 ], [1. , 0.02805611], [1. , 0.03006012], [1. , 0.03206413], [1. , 0.03406814], [1. , 0.03607214], [1. , 0.03807615]])&gt; . y=y.reshape(n,1) y[:20] . &lt;tf.Tensor: shape=(20, 1), dtype=float64, numpy= array([[3.35275314], [4.22611941], [4.87242315], [5.31953133], [3.8417442 ], [2.7444039 ], [3.75847533], [3.67882462], [3.84180216], [3.54850093], [4.77493479], [4.64718767], [4.2011182 ], [4.31115921], [4.81145538], [2.94308534], [4.03927822], [4.3866718 ], [3.96485239], [3.96641934]])&gt; . X.shape,y.shape . (TensorShape([500, 2]), TensorShape([500, 1])) . beta= tnp.array([-5.0,10.0]).reshape(2,1) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-5.], [10.]])&gt; . slope=-2*X.T @ y + 2*X.T @ X @ beta slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[0.], [0.]])&gt; . alpha=0.001 . step = -slope * alpha step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-0.], [-0.]])&gt; . for epoc in range(1000): slope= -2*X.T @ y + 2*X.T @ X @ beta step = -slope * alpha beta = beta + step . beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[4.04312335], [2.4858994 ]])&gt; .",
            "url": "https://ki5n2.github.io/charcoal/2022/03/30/DS.html",
            "relUrl": "/2022/03/30/DS.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "DS 5. 경사하강법, Grident Descent",
            "content": ". Data Science . lenture: Data Science_4-2nd week of lectures. | lenture date: 2022-03-28 | lecturer: Guebin choi | study date: 2022-03-29, 2022-03-30 | author: Kione kim | . . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#44221;&#49324;&#54616;&#44053;&#48277; - &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss= ( frac{1}{2} beta -1)^2$를 최소로 하는 $ beta$를 컴퓨터를 통해 구하는 문제를 생각해보자. (답은 당연히 $ beta=2$) . &#48169;&#48277; 2: Grid Descent . &#50508;&#44256;&#47532;&#51608; . (1) 임의의 초기값을 선정하고 loss를 계산한다. . $ beta=10 to loss(10)=(10/2-1)^2=16.0$ | . (10/2-1)**2 . 16.0 . (2) 임의의 초기값에서 좌우로 약간씩 이동해보고 loss를 계산한다. . 왼쪽으로 이동: $ beta=10.01, quad loss(10.01)=16.040025$ | 오른쪽으로 이동: $ beta=9.99, quad loss(9.99)=15.960025$ | . (10.01 /2 -1)**2 . 16.040025 . (9.99 /2 -1)**2 . 15.960025000000002 . (3) (2)의 결과를 보고 어느쪽으로 이동하는것이 유리한지 따져보고 유리한 방향으로 이동한다. . 유리한 방향: 절대값이 작은 쪽 . $ beta=9.99$ 로 이동 | . (4) (2)-(3) 의 과정을 반복한다. 왼쪽/오른쪽 모두 가봐도 유리한 지점이 없다면 알고리즘을 멈춘다. . &#50508;&#44256;&#47532;&#51608; &#44048;&#49345; . - 알고리즘이 멈추는 지점은 $ beta=2$이다. $ beta=2$일 때 loss가 0이 되므로 왼쪽으로 가도, 오른쪽으로 가도 현재 loss보다 크기 때문이다. . - 이 알고리즘은 grid search의 단점을 극복함! . $loss=(x beta-y)^2$의 꼴에서 $[-10,10]$ 이외의 지점에 해가 존재하여도 적절하게 해를 찾을 것. | 또한 비효율적으로 $ beta=2$ 이후에도 탐색을 반복하지 않음. | . &#50508;&#44256;&#47532;&#51608;&#54644;&#49437; . (1)의 의미: 임의의 초기값에서의 loss 계산(미분) (2)의 의미: 미분을 하라는 뜻 (3)의 의미: update (4)의 의미: 반복 . &#48120;&#48516;&#44228;&#49688;&#51032; &#51032;&#48120;&amp;&#49688;&#49885;&#54868; . - 미분계수의 의미 . 미분계수가 양수이다 -&gt; 왼쪽으로 이동해야 = 마이너스 0.01 | 미분계수가 음수이다 -&gt; 오른쪽으로 이동 = 플러스 0.01 | . - 수식화 . $$ beta_{next} = begin{cases} beta_{old} - 0.01 &amp; loss&#39;( beta_{old})&gt;0 beta_{old} + 0.01 &amp; loss&#39;( beta_{old})&lt;0 end{cases}$$ . 언제나 0.01씩 이동하는 것이 맞을까? . | loss 값의 절대값이 크면(찾아야 할 베타값으로 부터 거리가 먼 값이라면) 0.01씩 이동해서 언제 다 이동하지? . | 절대값이 크면(찾아야 하는 베타값으로부터 거리가 먼 경우) 이동을 많이 하고 절대값이 작으면(찾아야 하는 베타값으로부터 거리가 가까운 경우) 이동을 적게 하는 알고리즘으로 개선하고싶음! . | . beta = np.linspace(-10,5) plt.plot(beta,(beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x16d99c0e370&gt;] . - 위 그림에서 $ beta=-10$ 일 경우의 접선의 기울기는 $-6$이고 $ beta=-4$ 일때 접선의 기울기는 $-3$이다. . $ because loss = (0.5 beta-1)^2 to loss&#39; = 0.5 beta-1$ . $ beta=-10$에서 0.01만큼 이동했다면 $ beta=-4$에서 0.005만큼 이동해야 한다. | . - 개선한 알고리즘의 수식화 . $$ beta_{next} leftarrow beta_{old} - alpha left[ frac{ partial}{ partial beta} loss( beta) right]_{ beta= beta_{old}}$$ . 전의 수식이랑 좀 달라보이지만 $ beta_{old}$를 이동시켜 $ beta_{next}$를 만든다는 개념은 동일! | $ alpha&gt;0$ | $ alpha$의 의미: 한 번 업데이트할때 움직이는 보폭(걸음걸이), 절대값이 크면 큰 보폭으로 걷고 절대값이 작으면 작은 보폭으로 걷는다! | $ alpha= frac{0.01}{6}$ 로 만약 설정하면 $ beta=-10$일때 오른쪽으로 $0.01$움직임 | . 개선한 알고리즘을 이용한 풀이 . - with문 . beta= tf.Variable(-10.0) . with tf.GradientTape(persistent=True) as mytape: loss= (beta/2-1)**2 . mytape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . $beta=-10$일 때 기울기(미분계수)가 $-6$이 나옴 | . beta= tf.Variable(-4.0) . with tf.GradientTape(persistent=True) as mytape: loss= (beta/2-1)**2 . mytape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3.0&gt; . $beta=-4$일 떄 기울기(미분계수)가 $-3$이 나옴 | . iter1: $beta=-10$ 출발해서 $0.01$ 이동! . beta= tf.Variable(-10.0) . with tf.GradientTape(persistent=True) as mytape: loss= (beta/2-1)**2 . mytape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . alpha=0.01/6 . - -$α$이기 떄문에 beta.assign_sub를 이용! . beta.assign_sub(alpha*mytape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . 10에서 이동하기 시작하여 -9.99가 되었음 | . iter2: $beta=-9.99$ 출발해서 다시 $0.01$ 이동! . beta= tf.Variable(-9.99) . with tf.GradientTape(persistent=True) as mytape: loss= (beta/2-1)**2 . beta.assign_sub(alpha*mytape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.980008&gt; . $-9.99$에서 $0.01$이동하여 $-9.98$이 되었음 | . iter3: 반복! . - for문 . beta= tf.Variable(-10.0) for k in range(10000): with tf.GradientTape() as mytape: loss=(beta/2 -1)**2 beta.assign_sub(alpha*mytape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . $10000$번 반복해보았더니 $beta$가 이론적인 값인 $2$에 가까운 값($1.997$)이 나왔음! | . - $10000$번 반복이 아닌 $100$번 정도 반복해도 이론적인 값인 $2$에 가깝게 나올까? . beta= tf.Variable(-10.0) for k in range(100): with tf.GradientTape() as mytape: loss=(beta/2 -1)**2 beta.assign_sub(alpha*mytape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . $2$와 거리가 먼 $-9$ 값이 나옴 | . - 그렇다면 $1000$번은 어떻까? . beta= tf.Variable(-10.0) for k in range(1000): with tf.GradientTape() as mytape: loss=(beta/2 -1)**2 beta.assign_sub(alpha*mytape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-3.2133684&gt; . 아까보단 가까운 값($-3.2133$)이 나오긴 했음! . | 그런데 $1000$번을 반복해도 나오지 않은 건 좀...! . | 이는 설정한 alpha 값이 작기 때문! 즉 보폭이 작으니 많이 걸어야 하는 꼴..! . | . cf tf.Variable의 진가! . 왜 써야하는지 의문이 들었던 tf.Variable()가 점점 이해되기 시작함 | 안 그래도 제외된 기능이 많은데 assign_sub, assign_add와 같은 기능이 있는 이유 -&gt; 이것이 tf.Variable()의 목적이기 때문!! | tf.Variable()에 다른 기능들이 없는 이유는 tf.Variable()의 목적을 이루는데 필요하지 않은 기능들이기 때문! | tf.Gradient(persistent=False)가 default인 이유: 미분계산은 한 번하고 버리기 때문 ! | . &#54617;&#49845;&#47456; . - 목표: 아래의 학습과정을 시각화해보자. . beta = tf.Variable(-10.0) alpha=0.01/6 for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . &#49884;&#44033;&#54868; &#50696;&#48708;&#54617;&#49845; . - 설정 . from matplotlib import animation . plt.rcParams[&quot;animation.html&quot;]=&quot;jshtml&quot; . - 도화지(fig) &amp; 네모틀(axes) . fig = plt.figure() # plt.figure()는 그림 물체를 만드는 함수이고 그림 물체의 이름을 fig로 지정 . &lt;Figure size 432x288 with 0 Axes&gt; . type(fig) . matplotlib.figure.Figure . Figure라는 타입을 갖는 물체임! | . ax = fig.add_subplot() # fig.add_subplot는 네모틀(물체)을 만드는 함수이고 이를 ax(es)로 지정 . fig . - 도화지와 네모틀는 포함관계에 있음. . fig.axes . [&lt;AxesSubplot:&gt;] . fig.axes[0] # 원소가 추출됨 . &lt;AxesSubplot:&gt; . id(fig.axes[0]) . 1570293991936 . id(ax) . 1570293991936 . fig . 따라서 도화지를 출력하면 네모틀도 자동으로 출력됨 | . - 네모틀(ax)의 특수기능(=메소드)중에는 plot이 있음. 이것은 또 어떤 오브젝트를 생성함 . point = ax.plot([1,2,3],[3,4,5],&#39;or&#39;) point . [&lt;matplotlib.lines.Line2D at 0x2b175243f70&gt;] . point[0] . &lt;matplotlib.lines.Line2D at 0x16d9cd46280&gt; . point=point[0] point . &lt;matplotlib.lines.Line2D at 0x2b175243f70&gt; . 이는 다음 코드와 같다(튜플언패킹) | . point, = ax.plot([1,2,3],[3,4,5],&#39;or&#39;) point . &lt;matplotlib.lines.Line2D at 0x2b175263790&gt; . . (1,) . (1,) . 1+2 . 3 . (1,) + (2,) . (1, 2) . a,b = [1,2] . a . 1 . b . 2 . a, = [1] . a . 1 . . fig . point는 오브젝트 -&gt; x,y data를 변경해보자. | . point.get_xdata() . array([1, 2, 3]) . x data가 출력됨 | . point.get_ydata() . array([3, 4, 5]) . y data가 출력됨 | . point.set_ydata([4,4,4]) . y data가 변경됨! | . point.get_ydata() . [4, 4, 4] . fig . 왜 기존 데이터가 사라지지 않았지..? | . - 애니매이션 . def animate(i): if i%2 == 0: point.set_ydata([3,4,5]) else: point.set_ydata([4,4,4]) . ani=animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect 시각화 예비학습 끝 . . &#54617;&#49845;&#44284;&#51221; &#44396;&#54788;! . - 한 번에 구현하기 힘드니 간단한 그림을 먼저 구현해보자! . - 곡선위의 세 점의 변화를 시각화해보자 . beta_lst=[-10.0,-9.0,-8.0] loss_lst=[(-10.0/2 -1)**2, (-9.0/2 -1)**2, (-8.0/2 -1)**2] . fig=plt.figure() ax=fig.add_subplot() . - 곡선그리기 . _beta= np.linspace(-15,19) ax.plot(_beta,(_beta/2 -1)**2) . [&lt;matplotlib.lines.Line2D at 0x16d9ce29490&gt;] . fig . - 점을 찍기 . ax.plot(beta_lst[0],loss_lst[0],&#39;ro&#39;) fig . - 어떻게 애니메이션이 동작할지 정의하기 . point, = ax.plot(beta_lst[0], loss_lst[0], &#39;ro&#39;) fig . def animate(i): point.set_xdata(beta_lst[:(i+1)]) point.set_ydata(loss_lst[:(i+1)]) . ani=animation.FuncAnimation(fig,animate,frames=3) ani . &lt;/input&gt; Once Loop Reflect - 이제 이 과정을 학습해보자 beta = tf.Variable(-10.0) alpha=0.01/6 for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta . - 위 과정에 대한 애니메이션: 학습과정기록 . - 비어있는 리스트 . beta_lst=[] loss_lst=[] . - beta &amp; alpha 선언 . beta = tf.Variable(-10.0) alpha = 0.01/6 . - beta 넘파이화 . beta.numpy() . -10.0 . - for문에 쓰일 beta, loss 변경값 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2 -1)**2) . beta_lst . [-10.0] . loss_lst . [36.0] . - for문: 100번 반복 . for k in range(100): with tf.GradientTape() as mytape: loss=(beta/2 -1)**2 # 로스 식 beta.assign_sub(alpha*mytape.gradient(loss,beta)) # 베타값 변경 beta_lst.append(beta.numpy()) # 변경된 베타값 넘파이화한 후 베타 리스트에 저장 loss_lst.append((beta.numpy()/2 -1)**2) # 변경된 베타값에 대한 로스값 넘파이화 후 로스 리스트에 저장 . beta_lst[:20] . [-10.0, -9.99, -9.980008, -9.970025, -9.96005, -9.950083, -9.9401245, -9.930175, -9.920233, -9.910299, -9.900374, -9.890457, -9.8805485, -9.870648, -9.860756, -9.850872, -9.840997, -9.831129, -9.82127, -9.811419] . loss_lst[:20] . [36.0, 35.94002362785341, 35.88014867059451, 35.82037499958483, 35.76069678330009, 35.701119605823806, 35.64164333883673, 35.58226785413012, 35.5229873395956, 35.463807360728424, 35.40472778963908, 35.34574282873655, 35.28685802961354, 35.22807326469979, 35.16938275088614, 35.11079202589826, 35.052300962485106, 34.99390379198394, 34.935606038288824, 34.87740194234448] . 비어있던 베타리스트, 로스리스트에 값이 저장되어 있음 | . - 애니메이션 . fig= plt.figure() # fig 선언 ax= fig.add_subplot() . NameError Traceback (most recent call last) Input In [1], in &lt;cell line: 1&gt;() -&gt; 1 fig= plt.figure() # fig 선언 2 ax= fig.add_subplot() NameError: name &#39;plt&#39; is not defined . _beta= np.linspace(-15,19) # 애니메이션에 쓰일 곡선 그리기 ax.plot(_beta,(_beta/2 -1)**2) . [&lt;matplotlib.lines.Line2D at 0x16d9fda7a30&gt;] . point, = ax.plot(beta_lst[0], loss_lst[0], &#39;ro&#39;) # fig에 애니메이션에서 움직일 점 찍기 fig . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect 넘 느리다..! 학습율을 높여보자! | . - $α$가 $0.1$ 이면 어떨까? . beta_lst=[] loss_lst=[] . beta = tf.Variable(-10.0) alpha = 0.1 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2 -1)**2) . for k in range(100): with tf.GradientTape() as mytape: loss=(beta/2 -1)**2 # 로스 식 beta.assign_sub(alpha*mytape.gradient(loss,beta)) # 베타값 변경 beta_lst.append(beta.numpy()) # 변경된 베타값 넘파이화한 후 베타 리스트에 저장 loss_lst.append((beta.numpy()/2 -1)**2) # 변경된 베타값에 대한 로스값 넘파이화 후 로스 리스트에 저장 . fig= plt.figure() # fig 선언 ax= fig.add_subplot() . _beta= np.linspace(-15,19) # 애니메이션에 쓰일 곡선 그리기 ax.plot(_beta,(_beta/2 -1)**2) . [&lt;matplotlib.lines.Line2D at 0x16d9fe17580&gt;] . point, = ax.plot(beta_lst[0], loss_lst[0], &#39;ro&#39;) # fig에 애니메이션에서 움직일 점 찍기 fig . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect 아까보다 훨 나음! | 더 크게 하면 어떻게 될까? | . - 이전보다 α를 훨씬키워 3.0 정도가 되면 어떻게 될까? . beta_lst=[] loss_lst=[] . beta = tf.Variable(-10.0) alpha = 3.9 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2 -1)**2) . for k in range(100): with tf.GradientTape() as mytape: loss=(beta/2 -1)**2 # 로스 식 beta.assign_sub(alpha*mytape.gradient(loss,beta)) # 베타값 변경 beta_lst.append(beta.numpy()) # 변경된 베타값 넘파이화한 후 베타 리스트에 저장 loss_lst.append((beta.numpy()/2 -1)**2) # 변경된 베타값에 대한 로스값 넘파이화 후 로스 리스트에 저장 . fig= plt.figure() # fig 선언 ax= fig.add_subplot() . _beta= np.linspace(-15,19) # 애니메이션에 쓰일 곡선 그리기 ax.plot(_beta,(_beta/2 -1)**2) . [&lt;matplotlib.lines.Line2D at 0x16da0119fa0&gt;] . point, = ax.plot(beta_lst[0], loss_lst[0], &#39;ro&#39;) # fig에 애니메이션에서 움직일 점 찍기 fig . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect α가 너무 커 한 번에 반대 방향으로 이동했음 | 이 학습율 같은 경우는 비효율적이긴 하지만 최적의 베타값을 찾았으나, 이는 운이 좋은 경우임 ! | 다음 더 큰 α로 학습해보자! | . - α가 4.05 때 학습 . beta_lst=[] loss_lst=[] . beta = tf.Variable(-10.0) alpha = 4.05 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2 -1)**2) . for k in range(100): with tf.GradientTape() as mytape: loss=(beta/2 -1)**2 # 로스 식 beta.assign_sub(alpha*mytape.gradient(loss,beta)) # 베타값 변경 beta_lst.append(beta.numpy()) # 변경된 베타값 넘파이화한 후 베타 리스트에 저장 loss_lst.append((beta.numpy()/2 -1)**2) # 변경된 베타값에 대한 로스값 넘파이화 후 로스 리스트에 저장 . fig= plt.figure() # fig 선언 ax= fig.add_subplot() . _beta= np.linspace(-15,19) # 애니메이션에 쓰일 곡선 그리기 ax.plot(_beta,(_beta/2 -1)**2) . [&lt;matplotlib.lines.Line2D at 0x16da193ed90&gt;] . point, = ax.plot(beta_lst[0], loss_lst[0], &#39;ro&#39;) # fig에 애니메이션에서 움직일 점 찍기 fig . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect α가 너무 커 기울기의 절대값이 더 커지는 현상이 발생함! | 학습율이 너무 크면 문제가 생긴다 | . &#52628;&#44032;&#54617;&#49845; . 경사하강법을 이용하여 $y=(x-1)^2$의 최소값을 구하고 이를 애니메이션으로 시각화하라. (100번정도에 수렴하도록 적당한 학습률을 설정할것) . beta_lst=[] loss_lst=[] . beta = tf.Variable(5.0) # 반드시 소수형태로 표현! alpha = 0.025 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()-1)**2) . for k in range(100): with tf.GradientTape() as mytape: loss=(beta-1)**2 # 로스 식 beta.assign_sub(alpha*mytape.gradient(loss,beta)) # 베타값 변경 beta_lst.append(beta.numpy()) # 변경된 베타값 넘파이화한 후 베타 리스트에 저장 loss_lst.append((beta.numpy()-1)**2) # 변경된 베타값에 대한 로스값 넘파이화 후 로스 리스트에 저장 . fig= plt.figure() # fig 선언 ax= fig.add_subplot() . _beta= np.linspace(-4,6) # 애니메이션에 쓰일 곡선 그리기 ax.plot(_beta,(_beta-1)**2) . [&lt;matplotlib.lines.Line2D at 0x16da4f9c340&gt;] . point, = ax.plot(beta_lst[0], loss_lst[0], &#39;ro&#39;) # fig에 애니메이션에서 움직일 점 찍기 fig . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect $β$가 $5$에서 출발하여 $α=0.025$로 하여 $β=1$에 수렴 | $x=1$이다. | . beta_lst[100] . 1.0236822 .",
            "url": "https://ki5n2.github.io/charcoal/2022/03/29/DS.html",
            "relUrl": "/2022/03/29/DS.html",
            "date": " • Mar 29, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "DS 4. 미분, 경사하강법, grid search",
            "content": ". Data Science . lenture: Data Science_4-1nd week of lectures. | lenture date: 2022-03-23 | lecturer: Guebin choi | study date: 2022-03-23 | author: Kione kim | . . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp # tnp를 사용하면 넘파이에 익숙한 문법을 모두 쓸 수 있음 tnp.experimental_enable_numpy_behavior() # 기존에 생성된 tf.constant 자료형은 넘파이와 유사하게 동작한다. . &#48120;&#48516; . tf.GradientTape() . &#52852;&#54168;&#50696;&#51228; . - $x,y$ 선언 . x=tnp.array([20.1,22.2,22.7,23.3,24.4,25.1,26.2,27.3,28.4,30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy=array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])&gt; . tf.random.set_seed(50000) y=10.2+ 2.2*x+ tnp.random.randn(10) y . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([53.12550628, 59.48221878, 61.86480622, 64.06900254, 63.52340823, 62.85870759, 67.29683042, 69.54750897, 72.283444 , 76.08682149])&gt; . - 임의의 $β_0, β_1$ 값 설정 . beta0=tf.Variable([9.0]) beta1=tf.Variable([2.0]) . - with문 . with tf.GradientTape(persistent=True) as mytape: loss=sum((y-beta0-beta1*x)**2) mytape.gradient(loss,beta0), mytape.gradient(loss,beta1) . (&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([-119.87651], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3008.6094], dtype=float32)&gt;) . loss(잔차제곱합)를 β_0, β_1 각각에 대해 미분하여 대입한 값을 보았더니 0과는 거리가 멀다. 더 나은 직선이 많을 것 같다. . | 다른 베타값에 대한 미분값을 살펴보고 싶은데 위의 코드처럼 각각을 적용하는 것은 확장성이 떨어진다. . | 매트릭스 미분을 활용해보자! . | . - 매트릭스를 활용한 계산 . . &#48373;&#49845; . - 모형의 매트릭스화 . - 우리의 모형 . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . 이를 풀어서 쓰면 . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . 아래와 같다. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . 벡터와 매트릭스 형태로 표현하면 . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ . - 손실함수의 매트릭스화 . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . 이를 벡터로 표현하면, . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - 미분하는 과정의 매트릭스화 . loss를 최소화하는 ${ boldsymbol beta}$를 구해야하므로 loss를 ${ boldsymbol beta}$로 미분한식을 0이라고 놓고 풀면 된다. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . 따라서 $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$을 풀면 아래와 같다. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . . - 매트릭스로 $x,y$ 선언 . [1]*10 + [2]*10 . [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] . tnp.array([1]*10 + [2]*10).reshape(2,10).T . &lt;tf.Tensor: shape=(10, 2), dtype=int32, numpy= array([[1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2]])&gt; . x=tnp.array([1]*10 + [20.1,22.2,22.7,23.3,24.4,25.1,26.2,27.3,28.4,30.4]).reshape(2,10).T x . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]])&gt; . beta_true= tnp.array([[10.2,2.2]]) beta_true . &lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[10.2, 2.2]])&gt; . x@beta_true . ValueError Traceback (most recent call last) Input In [72], in &lt;cell line: 1&gt;() -&gt; 1 x@beta_true File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages six.py:718, in reraise(tp, value, tb) 716 value = tp() 717 if value.__traceback__ is not tb: --&gt; 718 raise value.with_traceback(tb) 719 raise value 720 finally: ValueError: Matrix size-incompatible: In[0]: [10,2], In[1]: [1,2] [Op:MatMul] . 메트릭스 선언을 잘못하면 오류가 남..! 잘못된 계산 | . beta_true= tnp.array([[10.2],[2.2]]) beta_true . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[10.2], [ 2.2]])&gt; . matrix 선언이 중요함,, | . x@beta_true . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[54.42], [59.04], [60.14], [61.46], [63.88], [65.42], [67.84], [70.26], [72.68], [77.08]])&gt; . tnp.random.seed(50000) y=x@beta_true + tnp.random.randn(10).reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[53.12550628], [59.48221878], [61.86480622], [64.06900254], [63.52340823], [62.85870759], [67.29683042], [69.54750897], [72.283444 ], [76.08682149]])&gt; . - 미분하고 싶은 $β$값 정의 . beta= tnp.array([[9.0],[2.0]]) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.], [2.]])&gt; . - with문 . with tf.GradientTape() as mytape: loss=(y-x@beta).T @ (y-x@beta) . mytape.gradient(loss,beta) . β가 tf.constant로 정의되었기 때문에 계산결과 Nan이 되었다. | . with tf.GradientTape() as mytape: mytape.watch(beta) loss=(y-x@beta).T @ (y-x@beta) . mytape.gradient(loss,beta) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -119.87650904], [-3008.60929929]])&gt; . 앞서 직접 구한 값과 동일하게 나왔다 | . - 다음과 같이 변환하여 보다 직관적으로 표현할 수 있다. . beta= tnp.array([[9.0],[2.0]]) with tf.GradientTape(persistent=True) as mytape: mytape.watch(beta) yhat=x@beta # yhat- 추정치 loss=(y-yhat).T @ (y-yhat) . mytape.gradient(loss,beta) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -119.87650904], [-3008.60929929]])&gt; . with tf.GradientTape() as mytape: mytape.watch(beta) yhat=x@beta # yhat- 추정치 loss=(y-yhat).T @ (y-yhat) ... ... ... 등등 : 계속해서 선형변환, 비선형변환 등등을 추가하여 계산할 수 있어 매우 편리하다고 한다... . - 이론적 풀이 . $loss&#39;( beta)= - {2 bf X}&#39; { bf y} + 2{ bf X}&#39; { bf X}{ boldsymbol beta} $ . -2 * x.T @ y + 2 * x.T @ x @ beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -119.87650904], [-3008.60929929]])&gt; . - 이론적인 $β$의 최적값을 찾아보고 (즉 $ hat beta$을 찾고) 그 지점에서 loss의 미분값(=접선의 기울기)을 구하라. 결과가 0인지 확인하라. 단 0은 길이가 2이고 각 원소가 0인 벡터이다. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . bhat = tf.linalg.inv(x.T @ x) @ x.T @ y # tf.linalg.inv()는 역행렬 구하는 기능 bhat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[13.49949892], [ 2.05974916]])&gt; . 앞서 구한 것보다 매우 근접한 값이 나왔다. | . with tf.GradientTape(persistent=True) as mytape: mytape.watch(bhat) yhat= x@bhat loss= (y-yhat).T @ (y-yhat) . mytape.gradient(loss,bhat) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1.87583282e-12], [-4.64863703e-11]])&gt; . bhat에서 접선의 기울기는 0과 매우 가까운 값이 계산되었다. | . &#44221;&#49324;&#54616;&#44053;&#48277; . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss= ( frac{1}{2} beta -1)^2$를 최소로 하는 $ beta$를 컴퓨터를 통해 구하는 문제를 생각해보자. (답은 당연히 $ beta=2$) . &#48169;&#48277; 1: grid search . 알고리즘: step1: beta를 적당한 범위 내 값들로 설정한다. step2: 각각의 범위 내 값들에 대한 loss를 계산한다. step3: loss값들 중 가장 작은 beta 값을 찾는다. . beta= tnp.linspace(-10,10,100) loss= (beta/2 -1)**2 . loss . &lt;tf.Tensor: shape=(100,), dtype=float64, numpy= array([3.60000000e+01, 3.47980818e+01, 3.36165697e+01, 3.24554637e+01, 3.13147638e+01, 3.01944700e+01, 2.90945822e+01, 2.80151005e+01, 2.69560249e+01, 2.59173554e+01, 2.48990919e+01, 2.39012346e+01, 2.29237833e+01, 2.19667381e+01, 2.10300990e+01, 2.01138659e+01, 1.92180390e+01, 1.83426181e+01, 1.74876033e+01, 1.66529946e+01, 1.58387920e+01, 1.50449954e+01, 1.42716049e+01, 1.35186205e+01, 1.27860422e+01, 1.20738700e+01, 1.13821039e+01, 1.07107438e+01, 1.00597898e+01, 9.42924191e+00, 8.81910009e+00, 8.22936435e+00, 7.66003469e+00, 7.11111111e+00, 6.58259361e+00, 6.07448220e+00, 5.58677686e+00, 5.11947760e+00, 4.67258443e+00, 4.24609734e+00, 3.84001632e+00, 3.45434139e+00, 3.08907254e+00, 2.74420977e+00, 2.41975309e+00, 2.11570248e+00, 1.83205795e+00, 1.56881951e+00, 1.32598714e+00, 1.10356086e+00, 9.01540659e-01, 7.19926538e-01, 5.58718498e-01, 4.17916539e-01, 2.97520661e-01, 1.97530864e-01, 1.17947148e-01, 5.87695133e-02, 1.99979594e-02, 1.63248648e-03, 3.67309458e-03, 2.61197837e-02, 6.89725538e-02, 1.32231405e-01, 2.15896337e-01, 3.19967350e-01, 4.44444444e-01, 5.89327620e-01, 7.54616876e-01, 9.40312213e-01, 1.14641363e+00, 1.37292113e+00, 1.61983471e+00, 1.88715437e+00, 2.17488011e+00, 2.48301194e+00, 2.81154984e+00, 3.16049383e+00, 3.52984389e+00, 3.91960004e+00, 4.32976227e+00, 4.76033058e+00, 5.21130497e+00, 5.68268544e+00, 6.17447199e+00, 6.68666463e+00, 7.21926334e+00, 7.77226814e+00, 8.34567901e+00, 8.93949597e+00, 9.55371901e+00, 1.01883481e+01, 1.08433833e+01, 1.15188246e+01, 1.22146720e+01, 1.29309254e+01, 1.36675849e+01, 1.44246505e+01, 1.52021222e+01, 1.60000000e+01])&gt; . . - 가장 작은 값을 찾는 함수 tf.argmin()이용 . tf.argmin([1,-1,2,-2]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt; . 가장 작은 값이 있는 인덱스를 출력해준다 | . . tf.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt; . beta[tf.argmin(loss)] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.9191919191919187&gt; . beta[59] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.9191919191919187&gt; . 이론적 값인 2와 근접한 값이 나왔음 | . beta[60] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.121212121212121&gt; . beta[58] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.7171717171717162&gt; . loss[59], loss[60], loss[58] . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.0016324864809713505&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.0036730945821854847&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.01999795939189892&gt;) . 앞 뒤로 한 개씩 차이나는 인덱스에 대한 loss값을 계산한 결과를 보니, 59번째 인덱스의 loss가 가장 작은 것을 알 수 있다. | . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - 문제1: local minimum . 지정한 범위 외에서 더 작은 loss값이 나올 수 있음. 즉, 지정한 범위 내에 global minimum이 존재한다는 보장이 없음 | . - 문제2: 효율적이지 않음 . 모든 loss를 찾는 건 비효율적일 수 있음. 초반 또는 중간에 $loss=0$인 값을 찾았음에도 계산을 멈추지 않고 (더 이상 탐색할 필요가 없음에도) 끝까지 계산함 | . &#52628;&#44032;&#54617;&#49845; . - 그리드서치를 이용하여 $(x-1)^2$을 최소화하는 $x$값을 구하여라. . beta= tnp.linspace(-5,5,1000) loss= (beta-1)**2 . tf.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=599&gt; . beta[599] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.9959959959959956&gt; . 이론적인 값인 1과 근접한 값이 나왔다 | .",
            "url": "https://ki5n2.github.io/charcoal/2022/03/23/DS.html",
            "relUrl": "/2022/03/23/DS.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "DS 3. Tensorflow 기본 문법(2), tf.GradientTape()",
            "content": ". Data Science . lenture: Data Science_3-2nd week of lectures. | lenture date: 2022-03-21 | lecturer: Guebin choi | study date: 2022-03-22 | author: Kione kim | . . import tensorflow as tf import numpy as np . tnp . tnp &#49324;&#50857;&#48176;&#44221; . - tf.constant는 쓰기 너무 어려움! 넘파이가 지원하는 편리한 기능을 사용할 수 없음 . 어려운 점 1: .reshape() 메소드 불가능/ tf.reshape()만 가능 . 어려운 점 2: .transpose(), .T 불가능/ tf.transpose()만 가능 . 어려운 점 3: 암묵적 형변환 불가능 . 어려운 점 4: (2,2) @ (2,) 연산 불가능 . 등등 . - 예시1: .reshape() . a=np.array([1,2,3,4]).reshape(2,2) a . array([[1, 2], [3, 4]]) . a=tf.constant([1,2,3,4]).reshape(2,2) . AttributeError Traceback (most recent call last) Input In [3], in &lt;cell line: 1&gt;() -&gt; 1 a=tf.constant([1,2,3,4]).reshape(2,2) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:508, in Tensor.__getattr__(self, name) 504 def __getattr__(self, name): 505 if name in {&#34;T&#34;, &#34;astype&#34;, &#34;ravel&#34;, &#34;transpose&#34;, &#34;reshape&#34;, &#34;clip&#34;, &#34;size&#34;, 506 &#34;tolist&#34;, &#34;data&#34;}: 507 # TODO(wangpeng): Export the enable_numpy_behavior knob --&gt; 508 raise AttributeError(&#34;&#34;&#34; 509 &#39;{}&#39; object has no attribute &#39;{}&#39;. 510 If you are looking for numpy-related methods, please run the following: 511 from tensorflow.python.ops.numpy_ops import np_config 512 np_config.enable_numpy_behavior()&#34;&#34;&#34;.format(type(self).__name__, name)) 513 self.__getattribute__(name) AttributeError: &#39;EagerTensor&#39; object has no attribute &#39;reshape&#39;. If you are looking for numpy-related methods, please run the following: from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . tf.reshape(tf.constant([1,2,3,4]),(2,2)) # 이렇게 써야함 . - 예시2: .transpose() . np.array([1,2,3,4]).transpose().reshape(2,2).T . array([[1, 3], [2, 4]]) . tf.constant([1,2,3,4]).transpose() . AttributeError Traceback (most recent call last) Input In [5], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([1,2,3,4]).transpose() File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:508, in Tensor.__getattr__(self, name) 504 def __getattr__(self, name): 505 if name in {&#34;T&#34;, &#34;astype&#34;, &#34;ravel&#34;, &#34;transpose&#34;, &#34;reshape&#34;, &#34;clip&#34;, &#34;size&#34;, 506 &#34;tolist&#34;, &#34;data&#34;}: 507 # TODO(wangpeng): Export the enable_numpy_behavior knob --&gt; 508 raise AttributeError(&#34;&#34;&#34; 509 &#39;{}&#39; object has no attribute &#39;{}&#39;. 510 If you are looking for numpy-related methods, please run the following: 511 from tensorflow.python.ops.numpy_ops import np_config 512 np_config.enable_numpy_behavior()&#34;&#34;&#34;.format(type(self).__name__, name)) 513 self.__getattribute__(name) AttributeError: &#39;EagerTensor&#39; object has no attribute &#39;transpose&#39;. If you are looking for numpy-related methods, please run the following: from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . tf.transpose(tf.constant([1,2,3,4])) # 이렇게 써야함 . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4])&gt; . tf.transpose(tf.reshape(tf.transpose(tf.constant([1,2,3,4])),(2,2))) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]])&gt; . 쓰기 불편,, | . - 예시3: 암묵적 형변환 . np.array([1,2,3])+np.array([1.1,2.2,3.3]) . array([2.1, 4.2, 6.3]) . tf.constant([1,2,3])+tf.constant([1.1,2.2,3.3]) . InvalidArgumentError Traceback (most recent call last) Input In [9], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([1,2,3])+tf.constant([1.1,2.2,3.3]) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . 암묵적 형변환 안 됨,, | . - 예시4: .max, .min 등등 넘파이에서 가능한 .max를 사용하려면 tf.constant에서는 tf.reduce_max()를 .min를 사용하려면 tf.reduce_min()를 사용해야함 . tf.reduce_max(tf.transpose(tf.reshape(tf.transpose(tf.constant([1,2,3,4])),(2,2)))) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=4&gt; . tf.reduce_min(tf.transpose(tf.reshape(tf.transpose(tf.constant([1,2,3,4])),(2,2)))) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . - 예시5: (2,2) @ (2,) 연산 . np.array([[1.0,2.0],[3.0,4.0]]) @ np.array([1,2]) . array([ 5., 11.]) . np.array([1,2]) @ np.array([[1.0,2.0],[3.0,4.0]]) . array([ 7., 10.]) . 차원이 맞지 않아도 넘파이에서는 알아서 잘 계산해줌 | . np.array([[1.0,2.0],[3.0,4.0]]) @ np.array([1,2]).reshape(2,1) . array([[ 5.], [11.]]) . np.array([1,2]).reshape(2,1) @ np.array([[1.0,2.0],[3.0,4.0]]) . ValueError Traceback (most recent call last) Input In [15], in &lt;cell line: 1&gt;() -&gt; 1 np.array([1,2]).reshape(2,1) @ np.array([[1.0,2.0],[3.0,4.0]]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . 헷갈릴 것 같다면 .reshape()를 써서 명시해줄 수 있다 | . tf.constant([[1.0,2.0],[3.0,4.0]]) @ tf.constant([1,2]) . InvalidArgumentError Traceback (most recent call last) Input In [16], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([[1.0,2.0],[3.0,4.0]]) @ tf.constant([1,2]) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:MatMul] . 차원이 맞지 않으면 계산을 하지 않는다. | . tnp &#49324;&#50857; . import tensorflow.experimental.numpy as tnp # tnp를 사용하면 넘파이에 익숙한 문법을 모두 쓸 수 있음 tnp.experimental_enable_numpy_behavior() # 기존에 생성된 tf.constant 자료형은 넘파이와 유사하게 동작한다. . tnp.array([1,2,3]) . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3])&gt; . tnp.diag([1,1]) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 1]])&gt; . &#53440;&#51077; . type(tf.constant([1,2])),type(tnp.array([1,2])) . (tensorflow.python.framework.ops.EagerTensor, tensorflow.python.framework.ops.EagerTensor) . tnp를 사용해도 타입은 변하지 않는다. 모두 EagerTensor | . - 앞서 본 어려운 점 1인 .reshape()가 사용 가능! . tf.constant([1,2,3,4]).reshape(2,2) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]])&gt; . - 어려운 점 2인 .transpose, .T 도 사용 가능! . tf.constant([1,2,3,4]).reshape(2,2).transpose().T . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]])&gt; . - 어려운 점 3인 암묵적 형변환도 가능! . tf.constant([1,2,3]) + tf.constant([1.1,2.2,3.3]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2.10000002, 4.20000005, 6.29999995])&gt; . - 어려운 점 4인 .max(), .min() 등도 가능! . tf.constant([1,2,3]) +tf.constant([1.1,2.2,3.3]).max() . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([4.29999995, 5.29999995, 6.29999995])&gt; . tf.constant([1,2,3]) + tf.constant([1.1,2.2,3.3]).min() . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2.10000002, 3.10000002, 4.10000002])&gt; . - 어려운 점 5인 (2,2) @ (2,)의 연산도 가능! . tnp.diag([1,1]) @ tf.constant([1,2]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2])&gt; . tf.constant([1,2]) @ tnp.diag([1,1]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2])&gt; . 많은 단점을 개선했음!! | . - 하지만 안 되는 것도 있음 . a = np.array([1,2,3]) a . array([1, 2, 3]) . a[0] = 0 a . array([0, 2, 3]) . a=tnp.array([1,2,3]) a . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3])&gt; . a[0]=0 a . TypeError Traceback (most recent call last) Input In [31], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=0 2 a TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . 이는 안 됨 | . tf.Variable . &#50696;&#48708;&#54617;&#49845; . a=8 a . 8 . id(a) . 140715972567008 . a=10 a . 10 . id(a) . 140715972567072 . 이는 값을 변경(편집)한 것이 아니라 재할당한 것이다. | . b=10 id(b) . 140715972567072 . 1) tf.constant는 메모리에 그 값을 올리는 것 -&gt; 메모리에 있는 것은 바꿀 수 없음 -&gt; 값을 변경하고자 하면 재할당 해야 함 . 2) tf.Variable은 메모리에 있는 값을 변경(편집)할 수 있음 . . - 선언 . tf.Variable([1,2]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([1, 2])&gt; . - type . type(tf.Variable([1,2])) . tensorflow.python.ops.resource_variable_ops.ResourceVariable . ResourceVariable 처음 보는 타입 | . - tf.constant() 선언 후 변환 . tf.Variable(tf.constant([1,2])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([1, 2])&gt; . - np.array() 선언 후 변환 . tf.Variable(np.array([1,2])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([1, 2])&gt; . - 인덱싱 . a=tf.Variable([1,2,3,4]) a[:2] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2])&gt; . - 연산 . tf.Variable([1,2]) + tf.Variable([2,1]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3])&gt; . 그런데 살펴보니, 인덱싱과 연산 후에 자료형이 tf.Variable에서 tf.Tensor로 바꼈다. | . a=tf.Variable([1,2]) b=tf.Variable([2,1]) type(a), type(b) . (tensorflow.python.ops.resource_variable_ops.ResourceVariable, tensorflow.python.ops.resource_variable_ops.ResourceVariable) . type(a+b) . tensorflow.python.framework.ops.EagerTensor . tf.Variable()로 만든 후 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐. | . tf.Variable([1,2]) + tf.Variable([1.1,2.2]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([2.10000002, 4.20000005])&gt; . 이는 원래 안 되지만, tnp.experimental_enable_numpy_behavior()를 선언하면 가능함 | . tf.Variable([1,2,3,4]).reshape(2,2) . AttributeError Traceback (most recent call last) Input In [46], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2,3,4]).reshape(2,2) AttributeError: &#39;ResourceVariable&#39; object has no attribute &#39;reshape&#39; . 이것은 또 안 됨.. | . tnp의 전부 되는 것은 아니고 일부만 가능 | . - tf.concat . a=tf.Variable([[1,2],[3,4]]) b=tf.Variable([[-1,-2],[-3,-4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, 2], [ 3, 4], [-1, -2], [-3, -4]])&gt; . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, -1, -2], [ 3, 4, -3, -4]])&gt; . - tf.stack . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[ 1, 2], [ 3, 4]], [[-1, -2], [-3, -4]]])&gt; . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[ 1, 2], [-1, -2]], [[ 3, 4], [-3, -4]]])&gt; . - 변수값 변경 가능 . a=tf.Variable([1,2]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([1, 2])&gt; . id(a) . 1561676107296 . a.assign_add([-1,2]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([0, 4])&gt; . id(a) . 1561676107296 . 위를 살펴보니, 메모리 주소가 같다. 즉 값이 재할당 된 것이 아닌 변경된 것임을 알 수 있다. | . : tf.constant()는 일반메모리에, tf.Variable은 GPU에 . &#48120;&#48516; . :Tensorflow를 사용하는 가장 큰 이유 . 모티브 . - 예제: 컴퓨터를 이용하여 $x=2$에서 $y=3x^2$의 접선의 기울기를 구해보자. . - (손풀이) . $ frac{dy}{dx}=6x$ 이므로, $x=2$를 대입 -&gt; 답 : 12. . - (컴퓨터로 풀이) . 도함수를 어떻게 구할지 모르겠으나 일단 $x=2$에서 접선의 기울기만 계산해보자 . step 1: 답만계산 . x1=2 y1=3*x1**2 . x2=2.000001 y2=3*x2**2 . (y2-y1)/(x2-x1) . 12.000003000266702 . step 2: 함수화 . def f(x): return 3*x**2 . def d(f,x): # python에서는 d(f,x)로 선언 가능 return (f(x+0.000001)-f(x))/0.000001 # f(x+a)-f(x)/a 꼴 . d(f,2) . 12.000003001944037 . d(f,3) . 18.000003002782705 . step 3: lambda 이용 . d(lambda x: 3*x**2,2) . 12.000003001944037 . d(lambda x: 3*x**2,3) . 18.000003002782705 . d(lambda x: x**2,3) . 6.000001000927568 . - 2개의 변수를 가지는 함수에 대한 미분 . def d(f,x): return (f(x+0.000001)-f(x))/0.000001 . def f(x,y): return x**2 + 3*y . d(f,(3,2)) # 미분을 하는 함수 d에 x**2+3*y를 적용시켜 미분시킨 후 x,y에 3,2를 각각 대입하라. . TypeError Traceback (most recent call last) Input In [68], in &lt;cell line: 1&gt;() -&gt; 1 d(f,(3,2)) Input In [66], in d(f, x) 1 def d(f,x): -&gt; 2 return (f(x+0.000001)-f(x))/0.000001 TypeError: can only concatenate tuple (not &#34;float&#34;) to tuple . 오류가 남 | . 이는 확장성이 떨어지기 때문에 다른 방법이 필요함 | . tf.GradientTape() . - 예제1: $x=2$에서 $y=3x^2$의 도함수 값을 구하여라. . x=tf.Variable([2.0]) a=tf.constant([3.0]) . . tf.GradientTape() . &lt;tensorflow.python.eager.backprop.GradientTape at 0x16b9b289130&gt; . 무엇인가 만들어졌음 | . mytape=tf.GradientTape() . dir(mytape) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__enter__&#39;, &#39;__eq__&#39;, &#39;__exit__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_ensure_recording&#39;, &#39;_persistent&#39;, &#39;_pop_tape&#39;, &#39;_push_tape&#39;, &#39;_recording&#39;, &#39;_tape&#39;, &#39;_tf_api_names&#39;, &#39;_tf_api_names_v1&#39;, &#39;_watch_accessed_variables&#39;, &#39;_watched_variables&#39;, &#39;batch_jacobian&#39;, &#39;gradient&#39;, &#39;jacobian&#39;, &#39;reset&#39;, &#39;stop_recording&#39;, &#39;watch&#39;, &#39;watched_variables&#39;] . dir()을 찍어보니 다음과 같은 게 있음 | . __enter__ | __exit__ | ?mytape.__enter__ . Signature: mytape.__enter__() Docstring: Enters a context inside which operations are recorded on this tape. File: c: users kko anaconda3 envs ds2022 lib site-packages tensorflow python eager backprop.py Type: method . 이는 함수이고 tape를 기록하는 기능을 함 | . ?mytape.__exit__ . Signature: mytape.__exit__(typ, value, traceback) Docstring: Exits the recording context, no further operations are traced. File: c: users kko anaconda3 envs ds2022 lib site-packages tensorflow python eager backprop.py Type: method . 함수인데 ( )안에 None, None, None를 입력해주어야 한다. 기록을 끄는 기능을 함 | . tf.GradientTape() 사용법: mytape.__enter__ 기록할 내용 mytape.__exit__ . x=tf.Variable([2.0]) # 미분하고 싶은 변수는 Variable로 할당 a=tf.constant([3.0]) # a는 상수로, 값을 저장하고 싶은 것은 constant로 할당 . mytape.__enter__() y=a*x**2 mytape.__exit__(None,None,None) . mytape.gradient(y,x) # 변수 x로 y를 미분 . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt; . - 예제2 . x=tf.Variable([2.0]) . mytape=tf.GradientTape() . mytape.__enter__() a=(x/2)*3 # a를 기록할 내용으로 가져왔음 y=a*x**2 mytape.__exit__(None,None,None) . mytape.gradient(y,x) # 변수 x로 y를 미분 . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([18.], dtype=float32)&gt; . a를 기록할 내용에서 지정해준 것 뿐인데 12가 아닌 18이 나왔음 | . 이는 다음의 과정을 통해 나온 결과임 | . $a= frac{3}{2}x$ . $y=ax^2= frac{3}{2}x^3$ . $ frac{dy}{dx}= frac{3}{2}3 x^2$ . 1.5 × 3 × 4 = 18 . 1.5 * 3 * 4 . 18.0 . 즉 y는 $a=3$을 기록한 것이 아니라, $a= frac{3}{2}x$를 기록한 것이다. | . - 테이프의 개념(중요) . - 상황 예시 . 미분계산을 컴퓨터에게 부탁하고 싶다. 예를 들어 $y=3x^2$에 대한 미분계산을 컴퓨터에 부탁 하기 위해서는 노트 및 연습장(=테이프)에 $y=3x^2$이라는 수식을 써서 보여줘야 한다. 이 때 컴퓨터에게 식($y$)이 무엇인지 그리고 무엇으로 미분하고 싶은지($x$)를 명시해야 한다. . - 비유 . (1) mytape = tf.GradientTape(): . tf.GradientTape() 는 컴퓨터에게 전달할 노트 생성 | mytape= 는 생성한 노트의 이름을 mytape이라고 지정 | . (2) mytape.__enter__(): . mytape 라는 노트를 연다. | . (3) a=x/2*3; y=a*x**2: . 컴퓨터에게 보여줄 식(미분하고자 하는 식) | . (4) mytape.__exit__(None,None,None): mytape라는 공책을 닫는다. . mytape 라는 노트를 닫는다. | . (5) mytape.gradient(y,x): $y$를 $x$로 미분한다는 메모를 남겨 컴퓨터에 전달 . - 여기서 가장 중요한 것은 노트를 언제 열고 닫는지이다 . 1) . x=tf.Variable([2.0]) a=x/2*3 # a=x*(3/2) -&gt; a=3 #2 mytape=tf.GradientTape() #3 mytape.__enter__() y=a*x**2 # y=3*x**2, a는 이미 3의 값을 가짐 mytape.__exit__(None,None,None) #4 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt; . 2) . x=tf.Variable([2.0]) #2 mytape=tf.GradientTape() #3 mytape.__enter__() a=x/2*3 y=a*x**2 # a=(3*x)/2로 y=(3*x)/2*x**2 mytape.__exit__(None,None,None) #4 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([18.], dtype=float32)&gt; . 이를 유심히 살펴보니, 코드가 뭔가 깔끔하지가 않음. 중복되는 것도 있어 보임. . | 보다 깔끔하고 효율적인 코드가 필요! -&gt; with문활용 . | . - 참고: x=tf.constant([2]) float형으로 입력하지 않으면 오류!! tf.Variable([2])도 마찬가지 . x=tf.constant([2]) #2 mytape=tf.GradientTape() #3 mytape.__enter__() a=x/2*3 y=a*x**2 # a=(3*x)/2로 y=(3*x)/2*x**2 mytape.__exit__(None,None,None) #4 mytape.gradient(y,x) . WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32 . x=tf.Variable([2]) #2 mytape=tf.GradientTape() #3 mytape.__enter__() a=x/2*3 y=a*x**2 # a=(3*x)/2로 y=(3*x)/2*x**2 mytape.__exit__(None,None,None) #4 mytape.gradient(y,x) . WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32 . with&#47928;&#51012; &#54876;&#50857;&#54620; tf.GradientTape() . with문 사용법: with tf.GradientTape() as mytape: 컴퓨터에게 전달할 수식 1. tf.GradientTape()를 실행하면 오브젝트가 하나 생성되는데 이를 mytape라고 지정한다. 기존에 mytape=tf.GradientTape()을 with tf.GradientTape as mytape으로 지정해준다. 2. with문이 시작되면서 mytape.__enter__()이 실행 3. 컴퓨터에게 전달한 수식 실행 4. with문이 끝나면서 mytape.__exit__(None,None,None)가 실행 . - with문 사용한 예제 풀이 . 1) . x=tf.Variable([2.0]) a=x/2*3 with tf.GradientTape() as mytape: y=a*x**2 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt; . 2) . x=tf.Variable([2.0]) with tf.GradientTape() as mytape: a=x/2*3 y=a*x**2 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([18.], dtype=float32)&gt; . - persistent=True: 계산한 값을 버리지 않는 기능으로 계속해서 출력할 수 있다 . x=tf.Variable([2.0]) a=x/2*3 with tf.GradientTape() as mytape: y=a*x**2 . mytape.gradient(y,x),mytape.gradient(y,x) . RuntimeError Traceback (most recent call last) Input In [115], in &lt;cell line: 1&gt;() -&gt; 1 mytape.gradient(y,x),mytape.gradient(y,x) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python eager backprop.py:1029, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients) 999 &#34;&#34;&#34;Computes the gradient using operations recorded in context of this tape. 1000 1001 Note: Unless you set `persistent=True` a GradientTape can only be used to (...) 1026 called with an unknown value. 1027 &#34;&#34;&#34; 1028 if self._tape is None: -&gt; 1029 raise RuntimeError(&#34;A non-persistent GradientTape can only be used to &#34; 1030 &#34;compute one set of gradients (or jacobians)&#34;) 1031 if self._recording: 1032 if not self._persistent: RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians) . 두 번 실행하면 오류가 난다 | . x=tf.Variable([2.0]) a=x/2*3 with tf.GradientTape(persistent=True) as mytape: y=a*x**2 . mytape.gradient(y,x),mytape.gradient(y,x) . (&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt;) . 오류 해결! | . - 미분계산에 사용할 변수($x$)를 tf.constant로 설정하면 계산이 되지 않는다. . x=tf.constant([2.0]) a=x/2*3 with tf.GradientTape(persistent=True) as mytape: y=a*x**2 mytape.gradient(y,x) . print(mytape.gradient(y,x)) . None . 미분계산에 사용할 변수는 tf.Variable로 설정해야 한다 | . - 자동감시모드 &amp; 수동감시모드 . - 예시 . notename.watch(x):를 사용하면 미분계산에 사용할 변수($x$)를 tf.constant()로 설정하였더라도 계산이 된다. . x=tf.constant([2.0]) a=x/2*3 with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) y=a*x**2 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt; . 계산이 되었음 ! | . tf.Variable(): 자동감시모드 tf.constant(): 감시 X `notename.watch(감시할 변수)`사용: (감시할 변수에 대한) 수동감시모드 . - 자동감시모드(기존) . x=tf.Variable([2.0]) a=x/2*3 with tf.GradientTape(persistent=True) as mytape: y=a*x**2 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt; . - 자동감시모드 off : watch_accessed_variables=False 사용 . x=tf.Variable([2.0]) a=x/2*3 with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: y=a*x**2 mytape.gradient(y,x) . - 수동감시모드 . x=tf.constant([2.0]) a=x/2*3 with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) # 수동감시 y=a*x**2 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt; . - 자동감시모드 -&gt; 자동감시모드 off -&gt; 수동감시 ON . x=tf.Variable([2.0]) a=x/2*3 with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: mytape.watch(x) y=a*x**2 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)&gt; . 가능! | . 헷갈리면 언제나 mytape.watch(변수)를 명시해도 괜찮음! | 오류가 나는 것은 아니니! | . &#52628;&#44032;&#54617;&#49845; . - tf.GradientTape( )를 이용하여 $y=x^2$에서 $x=0$에서의 접선의 기울기를 구하라. . x=tf.constant([0.0]) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) y=x**2 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)&gt; .",
            "url": "https://ki5n2.github.io/charcoal/2022/03/22/DS.html",
            "relUrl": "/2022/03/22/DS.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "DS 2. Tensorflow 기본 문법(1) - tf.constant",
            "content": ". Data Science . lenture: Data Science_2-2, 3-1nd week of lectures. | lenture date: 2022-03-14, 2022-03-16 | lecturer: Guebin choi | study date: 2022-03-18, 2022-03-20 | author: Kione kim | . . Tensorflow &#49324;&#50857;&#48277; . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . [] . NO GPU | . tf.constant . &#50696;&#48708;&#54617;&#49845;: &#51473;&#52393;&#47532;&#49828;&#53944; . [1,[2,3]] . [1, [2, 3]] . _[1][1] . 3 . lst = [[1,2],[3,4]] lst . [[1, 2], [3, 4]] . lst[0][1] . 2 . lst[1][0] . 3 . 1 2 3 4 . print(lst[0][0]) # (1,1) print(lst[0][1]) # (1,2) print(lst[1][0]) # (2,1) print(lst[1][1]) # (2,2) . 1 2 3 4 . 이는 벡터일 뿐인데 매트릭스처럼 보임(매트릭스는 아님) | . - 이를(매트릭스처럼 보이는 것을) 활용하여 행렬을 만들어보자 . $4×2$ . lst = [[1,2],[3,4],[5,6],[7,8]] lst . [[1, 2], [3, 4], [5, 6], [7, 8]] . $4×1$ . lst = [[1],[2],[3],[4]] lst # 길이가 4인 column-vector처럼 보임 . [[1], [2], [3], [4]] . $2×4$ . lst = [[1,2,3,4],[5,6,7,8]] lst . [[1, 2, 3, 4], [5, 6, 7, 8]] . $1×4$ . lst = [[1,2,3,4]] lst # 길이가 4인 row-vector처럼 보임 . [[1, 2, 3, 4]] . - 3차원 . lst = [[[1,2],[3,4]],[[5,6],[7,8]]] # 2×2×2 매트릭스처럼 보임 lst . [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] . print(lst[0][0][0]) print(lst[0][0][1]) print(lst[0][1][0]) print(lst[0][1][1]) # 차원 2 print(lst[1][0][0]) print(lst[1][0][1]) print(lst[1][1][0]) print(lst[1][1][1]) . 1 2 3 4 5 6 7 8 . &#49440;&#50616; . - 스칼라 . _scalar = tf.constant(1) _scalar . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . - 벡터 . [1,2,3] # 리스트 . [1, 2, 3] . _vector = tf.constant([1,2,3]) # 리스트를 tf.constant()함수를 사용하여 벡터로 _vector . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3])&gt; . _vector[-1], _vector[-2] . (&lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;) . - 아래 두 코드는 다름. 두 번째 코드는 의도하지 않은 결과임 . a=tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4])&gt; . a=tf.constant(1,2,3,4) a . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 1., 1.])&gt; . tf.constant([ ]) [ ]를 사용해주어야 함 ! | . - 매트릭스 . _mat = tf.constant([[1,2,3],[4,5,6]]) _mat . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [4, 5, 6]])&gt; . 하나의 행렬로 묶을 때는 행렬처음과 끝에 [ ]로 받아주어야 한다. | . _mat[0] . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3])&gt; . _mat[0][0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . _mat[0,0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . _mat[0][0]를 _mat[0,0]와 같이 사용할 수 있다 | . - 첫번째 행을 뽑는 방법 . - 방법1 . _mat[0] . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3])&gt; . - 방법2 . _mat[0,:] . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3])&gt; . - 첫번째 열을 뽑는 방법 . _mat[:,0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 4])&gt; . _mat[:,-1] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 6])&gt; . 즉 행과 열을 모두 뽑을 수 있는 방법은 방법2이다. 확장성이 있음 | . type(_scalar) . tensorflow.python.framework.ops.EagerTensor . EagerTensor임을 기억 | . type(_vector) . tensorflow.python.framework.ops.EagerTensor . tf.constant&#51032; &#48520;&#54200;&#54620;&#51216; . 1. dtype 모든 원소가 똑같아야함 2. 값을 바꿀 수 없음 3. dtype이 다르면 연산이 불가능 . 2. 값을 바꿀 수 없음 . _mat[0,0] = 10 . TypeError Traceback (most recent call last) Input In [41], in &lt;cell line: 1&gt;() -&gt; 1 _mat[0,0] = 10 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . 3. dtype이 다르면 연산 불가능 . tf.constant([1,2])+ tf.constant([3,4]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6])&gt; . tf.constant([1.1,2])+ tf.constant([3,4]) . InvalidArgumentError Traceback (most recent call last) Input In [43], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([1.1,2])+ tf.constant([3,4]) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] . tf.constant([1.1,2]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.1, 2. ], dtype=float32)&gt; . tf.constant([3,4]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4])&gt; . 뜯어보니, tf.constant([1.1,2])는 float형이고 tf.constant([3,4])는 int형이다. float과 int가 달라서 계산이 안 된 것,, ? OMG | . - 다음과 같이 해야 한다 . tf.constant([1.1,2])+tf.constant([3.,4.]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.1, 6. ], dtype=float32)&gt; . tf.constant([1.1,2])+tf.constant([3,4.]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.1, 6. ], dtype=float32)&gt; . tf.constant([1.1,2])+tf.constant([3.,4]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.1, 6. ], dtype=float32)&gt; . int형에 있는 하나의 원소에 .을 찍어주면 float형이 된다. | . - 같은 float형이라도 불가능한 경우가 있음 . tf.constant([1.1,2]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.1, 2. ], dtype=float32)&gt; . tf.constant([3.0,4.0],dtype=tf.float64) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3., 4.])&gt; . tf.constant([1.1,2])+tf.constant([3.0,4.0],dtype=tf.float64) . InvalidArgumentError Traceback (most recent call last) Input In [56], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([1.1,2])+tf.constant([3.0,4.0],dtype=tf.float64) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] . float32와 float64가 달라서 오류,,,? OMG2 | . tf.constant $ to$ &#45336;&#54028;&#51060; . _vector = tf.constant([1,2,3,4]) _vector . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4])&gt; . np.array(_vector) # 방법1 . array([1, 2, 3, 4]) . _vector.numpy() . array([1, 2, 3, 4]) . Tensorflow 내에서 numpy를 쓸 수 있다 | . &#50672;&#49328; . - 더하기 . a=tf.constant([1,2,3]) b=tf.constant([4,5,6]) a+b . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9])&gt; . tf.add(a,b) . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9])&gt; . - 곱하기 . a=tf.constant([[1,2],[3,4]]) b=tf.constant([[5,6],[7,8]]) a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]])&gt; . tf.multiply(a,b) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]])&gt; . 각 원소끼리 곱 | . - 행렬곱 . a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,0],[0,1]]) a@b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]])&gt; . tf.matmul(a,b) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]])&gt; . - 역행렬 . a=tf.constant([[1,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 2]])&gt; . tf.linalg.inv(a) . InvalidArgumentError Traceback (most recent call last) Input In [69], in &lt;cell line: 1&gt;() -&gt; 1 tf.linalg.inv(a) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python ops gen_linalg_ops.py:1505, in matrix_inverse(input, adjoint, name) 1503 return _result 1504 except _core._NotOkStatusException as e: -&gt; 1505 _ops.raise_from_not_ok_status(e, name) 1506 except _core._FallbackException: 1507 pass File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . 당연히 되어야하는데 오류가 남. 이는 a가 int이지만, 역행렬 값은 0.5(소수값)이 나오기 때문.. OMG3 . | 따라서 다음과 같이 하면 가능.. . | . a=tf.constant([[1.0,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 0.], [0., 2.]], dtype=float32)&gt; . tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1. , 0. ], [0. , 0.5]], dtype=float32)&gt; . a도 float형으로 했기 때문에 가능 | . a @ tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 0.], [0., 1.]], dtype=float32)&gt; . - 행렬식, 대각합 . a=tf.constant([[1.0,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 0.], [0., 2.]], dtype=float32)&gt; . tf.linalg.det(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt; . tf.linalg.trace(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt; . - 집계함수 . a=tf.constant([[1,2,3],[4,5,6]]) a . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [4, 5, 6]])&gt; . tf.reduce_sum(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=21&gt; . tf.sum(a) # 잘못된 코드1 . AttributeError Traceback (most recent call last) Input In [82], in &lt;cell line: 1&gt;() -&gt; 1 tf.sum(a) AttributeError: module &#39;tensorflow&#39; has no attribute &#39;sum&#39; . a.sum(a) # 잘못된 코드2 . AttributeError Traceback (most recent call last) Input In [56], in &lt;cell line: 1&gt;() -&gt; 1 a.sum(a) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:513, in Tensor.__getattr__(self, name) 505 if name in {&#34;T&#34;, &#34;astype&#34;, &#34;ravel&#34;, &#34;transpose&#34;, &#34;reshape&#34;, &#34;clip&#34;, &#34;size&#34;, 506 &#34;tolist&#34;, &#34;data&#34;}: 507 # TODO(wangpeng): Export the enable_numpy_behavior knob 508 raise AttributeError(&#34;&#34;&#34; 509 &#39;{}&#39; object has no attribute &#39;{}&#39;. 510 If you are looking for numpy-related methods, please run the following: 511 from tensorflow.python.ops.numpy_ops import np_config 512 np_config.enable_numpy_behavior()&#34;&#34;&#34;.format(type(self).__name__, name)) --&gt; 513 self.__getattribute__(name) AttributeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object has no attribute &#39;sum&#39; . tf.reduce_max(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=6&gt; . tf.reduce_min(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . tf.reduce_mean(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt; . 왜 평균이 3.5가 아닌 3이 나오지? | . - 행렬곱 고급 . _I=tf.constant([[1.0,0.0],[0.0,1.0]]) _I . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 0.], [0., 1.]], dtype=float32)&gt; . _x=tf.constant([11,22]) _x . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([11, 22])&gt; . _I @ _x . InvalidArgumentError Traceback (most recent call last) Input In [91], in &lt;cell line: 1&gt;() -&gt; 1 _I @ _x File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:MatMul] . 오류가 남. | 이유: _x가 길이가 2인 열벡터도 아니고 길이가 2인 행벡터도 아니기 때문, _x는 그저 길이가 2인 벡터임 | . _x @ _I . InvalidArgumentError Traceback (most recent call last) Input In [92], in &lt;cell line: 1&gt;() -&gt; 1 _x @ _I File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:MatMul] . 이것 또한 마찬가지, 열벡터가 아니기 때문 | . - 다음과 같이 표현해주어야 함 . - 열벡터 선언 . _x=tf.constant([[11],[22]]) _x . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[11], [22]])&gt; . - cf. 행벡터 . _x=tf.constant([[11,22]]) _x . &lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[11, 22]])&gt; . _I @ _x . InvalidArgumentError Traceback (most recent call last) Input In [101], in &lt;cell line: 1&gt;() -&gt; 1 _I @ _x File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:MatMul] . float형으로 선언해주지 않아서 난 오류 | . _x=tf.constant([[11.0],[22.0]]) _x . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[11.], [22.]], dtype=float32)&gt; . _I @ _x . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[11.], [22.]], dtype=float32)&gt; . - 행벡터 선언 tf.constant([[ , ]]) . _x=tf.constant([[11.0,22.0]]) _x . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[11., 22.]], dtype=float32)&gt; . _x @ _I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[11., 22.]], dtype=float32)&gt; . &#54805;&#53468;&#48320;&#54872; . - 기본 . a=tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4])&gt; . 이는 길이가 4인 벡터 | . tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]])&gt; . tf.reshape(a,(4,1)) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]])&gt; . tf.reshape(a,(1,4)) . &lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]])&gt; . - 응용 -1이용 : -1은 자동 연산하여 값을 채워주는 기능 . a=tf.constant([0,1,2,3,4,5,6,7,8,9,10,11]) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])&gt; . tf.reshape(a,(4,3)) # 기본 . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]])&gt; . tf.reshape(a,(4,-1)) # 응용 . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]])&gt; . tf.reshape(a,(2,2,3)) # 기본 . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]])&gt; . tf.reshape(a,(2,2,-1)) # 응용 . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]])&gt; . tf.reshape(a,(3,2,2)) # 기본 . &lt;tf.Tensor: shape=(3, 2, 2), dtype=int32, numpy= array([[[ 0, 1], [ 2, 3]], [[ 4, 5], [ 6, 7]], [[ 8, 9], [10, 11]]])&gt; . tf.reshape(a,(3,2,-1)) . &lt;tf.Tensor: shape=(3, 2, 2), dtype=int32, numpy= array([[[ 0, 1], [ 2, 3]], [[ 4, 5], [ 6, 7]], [[ 8, 9], [10, 11]]])&gt; . tf.reshape(a,(2,-1)) . &lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy= array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]])&gt; . $2×6$ 행렬로 만들어줌, 즉, 3차원으로 만들고 싶을 땐 두 번째 값까지는 채워줘야 함 | . - reshape한 것을 다시 reshape 해줄 수 있음 . - reshape한 것을 1차원으로 다시 만들어보자 . b=tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]])&gt; . tf.reshape(b,12) # 기본 . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])&gt; . tf.reshape(b,-1) # 응용 . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])&gt; . &#49440;&#50616;&#44256;&#44553; . - 대각행렬선언 . - 넘파이로 먼저 만들고 tf.constant( )를 사용하여 자료형 변환 . np.diag([1,2,3]) . array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) . tf.constant(np.diag([1,2,3])) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . - 1으로만 이루어진 텐서 만들기 . tf.ones((3,4)) . &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=float32)&gt; . tf.ones([3,4]) . &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=float32)&gt; . tf.reshape([1]*12,(3,4)) . &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]])&gt; . tf.reshape(tf.constant([1]*12),(3,4)) . &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]])&gt; . 같은 결과 | . - 0으로만 이루어진 텐서 만들기 . tf.zeros([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . tf.zeros((3,3)) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . - 리스트로 변환 후 텐서로 변환하지 않고 바로 텐서로 바꾸기 : 즉, a $ to$ list $ to$ tensor - X a $ to$ tensor - O . range(12) . range(0, 12) . a=range(0,12) a? . Type: range String form: range(0, 12) Length: 12 Docstring: range(stop) -&gt; range object range(start, stop[, step]) -&gt; range object Return an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step. range(i, j) produces i, i+1, i+2, ..., j-1. start defaults to 0, and stop is omitted! range(4) produces 0, 1, 2, 3. These are exactly the valid indices for a list of 4 elements. When step is given, it specifies the increment (or decrement). . range? 가 무엇인지는 모르겠지만, 일단 리스트로 바꿔보자. | . list(a) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] . list 형태로 변환되었다. | . - 동일한 논리로 range 가 무엇인지는 모르겠지만, tf.constant를 이용하여 텐서형태로 바꿔보자. (tf.constant가 텐서로 변환해주는 기능) . tf.constant(range(0,12)) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])&gt; . tf.constant(range(1,12)) # 1부터 시작 . &lt;tf.Tensor: shape=(11,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])&gt; . tf.constant(range(2,6)) . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([2, 3, 4, 5])&gt; . tf.constant(range(2,21,2)) # 2칸씩 점프 . &lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 2, 4, 6, 8, 10, 12, 14, 16, 18, 20])&gt; . tf.constant(range(2,21,3)) # 3칸씩 점프 . &lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 2, 5, 8, 11, 14, 17, 20])&gt; . - 등간격 수열 만들기 . tf.linspace(0,1,14) # 0 ~ 1 까지 길이가 14인 수열 만들기 . &lt;tf.Tensor: shape=(14,), dtype=float64, numpy= array([0. , 0.07692308, 0.15384615, 0.23076923, 0.30769231, 0.38461538, 0.46153846, 0.53846154, 0.61538462, 0.69230769, 0.76923077, 0.84615385, 0.92307692, 1. ])&gt; . tf.linspace(-1,20,14) # -1 ~ 20 까지 길이가 14인 수열 . &lt;tf.Tensor: shape=(14,), dtype=float64, numpy= array([-1. , 0.61538462, 2.23076923, 3.84615385, 5.46153846, 7.07692308, 8.69230769, 10.30769231, 11.92307692, 13.53846154, 15.15384615, 16.76923077, 18.38461538, 20. ])&gt; . tf.linspace([0,-1],[14,20],14) # 첫 열: 0~14까지, 두 번째 열: -1~20까지 길이가 14인 수열 . &lt;tf.Tensor: shape=(14, 2), dtype=float64, numpy= array([[ 0. , -1. ], [ 1.07692308, 0.61538462], [ 2.15384615, 2.23076923], [ 3.23076923, 3.84615385], [ 4.30769231, 5.46153846], [ 5.38461538, 7.07692308], [ 6.46153846, 8.69230769], [ 7.53846154, 10.30769231], [ 8.61538462, 11.92307692], [ 9.69230769, 13.53846154], [10.76923077, 15.15384615], [11.84615385, 16.76923077], [12.92307692, 18.38461538], [14. , 20. ]])&gt; . - $14×2$가 아닌 $2×14$ 행렬은? . : axis=1사용 . tf.linspace([0,-1],[1,20],14,axis=1) . &lt;tf.Tensor: shape=(2, 14), dtype=float64, numpy= array([[ 0. , 0.07692308, 0.15384615, 0.23076923, 0.30769231, 0.38461538, 0.46153846, 0.53846154, 0.61538462, 0.69230769, 0.76923077, 0.84615385, 0.92307692, 1. ], [-1. , 0.61538462, 2.23076923, 3.84615385, 5.46153846, 7.07692308, 8.69230769, 10.30769231, 11.92307692, 13.53846154, 15.15384615, 16.76923077, 18.38461538, 20. ]])&gt; . tf.linspace([0,-1],[1,20],14,axis=0) . &lt;tf.Tensor: shape=(14, 2), dtype=float64, numpy= array([[ 0. , -1. ], [ 0.07692308, 0.61538462], [ 0.15384615, 2.23076923], [ 0.23076923, 3.84615385], [ 0.30769231, 5.46153846], [ 0.38461538, 7.07692308], [ 0.46153846, 8.69230769], [ 0.53846154, 10.30769231], [ 0.61538462, 11.92307692], [ 0.69230769, 13.53846154], [ 0.76923077, 15.15384615], [ 0.84615385, 16.76923077], [ 0.92307692, 18.38461538], [ 1. , 20. ]])&gt; . - 랜덤 . tf.random.normal([10]) . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([-2.031805 , 0.0878811 , -0.08389507, -0.4712213 , 1.7228402 , 1.5234696 , -0.45930204, 0.27446908, 0.6777201 , 1.2311221 ], dtype=float32)&gt; . tf.random.normal(10) . InvalidArgumentError Traceback (most recent call last) Input In [131], in &lt;cell line: 1&gt;() -&gt; 1 tf.random.normal(10) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: shape must be a vector of {int32,int64}, got shape [] [Op:RandomStandardNormal] . 위와 같이 하면 오류가 남 | . tf.random.normal([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[-0.67909116, 1.6647867 , -1.1921434 ], [-0.41828915, -1.007226 , -0.37517777], [-0.35723418, -0.90447557, -0.21201086]], dtype=float32)&gt; . tf.random.uniform([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0.49846053, 0.53244936, 0.37250876], [0.5088717 , 0.01882815, 0.50129974], [0.8890096 , 0.7716527 , 0.68743026]], dtype=float32)&gt; . - numpy를 이용한 자료형 변환 . a=np.random.randn(10) a . array([ 1.42470939, -0.43431904, 0.35631914, -0.83401698, 0.98403914, 0.03123622, 0.3625249 , 0.45672775, 0.51236327, 0.74739683]) . list(a) # 일단 리스트로 변환해 봄 . [1.4247093944590954, -0.4343190394693054, 0.35631913687969213, -0.8340169822900257, 0.9840391372400604, 0.031236220641082953, 0.36252489894658657, 0.4567277510989872, 0.5123632705088501, 0.7473968310235103] . tf.constant(a) . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([ 1.42470939, -0.43431904, 0.35631914, -0.83401698, 0.98403914, 0.03123622, 0.3625249 , 0.45672775, 0.51236327, 0.74739683])&gt; . - tf.random이 numpy에 있는 모든 기능을 구현하는 것은 아님. . np.random.randn(10) . array([-0.37923556, 1.89366845, -0.60768019, 0.89680048, -0.11687065, -0.67507322, 3.45690649, 0.13774121, 0.5305251 , -0.37030568]) . tf.random.randn([10]) . AttributeError Traceback (most recent call last) Input In [143], in &lt;cell line: 1&gt;() -&gt; 1 tf.random.randn([10]) AttributeError: module &#39;tensorflow._api.v2.random&#39; has no attribute &#39;randn&#39; . tf.concat . tf.concat 사용법: 축 지정이 매우 헷갈리기 때문에 외우는 게 필요함 python은 0부터 시작하기 때문에 첫 번째 축이 0을 의미, 두 번째 축이 1을 의미, 세 번째 축이 2를 의미한다. (2차원에선) 함수 축을 그릴 때 가로를 그린 후 세로를 그리기 때문에 첫 번째 축이 가로, 두 번째 축이 세로를 의미한다. (2차원에선) 따라서 axis=0은 가로로 쌓는다는 것(아래로 계속해서)을 의미하고 axis=1은 세로로 쌓는다는 것(옆으로 계속해서)을 의미한다. 3차원부터는 앞서 정리한 것처럼 첫 번째 축-0, 두 번째 축-1, 세 번째 축-2 으로 외우는 것이 좋다. . - $2×1$ 벡터와 $2×1$ 벡터를 concat해서 $2×2$ 벡터로 만든다 . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) . a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]])&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]])&gt;) . tf.concat([a,b]) . TypeError Traceback (most recent call last) Input In [151], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b]) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util dispatch.py:1076, in add_dispatch_support.&lt;locals&gt;.decorator.&lt;locals&gt;.op_dispatch_handler(*args, **kwargs) 1074 if iterable_params is not None: 1075 args, kwargs = replace_iterable_params(args, kwargs, iterable_params) -&gt; 1076 result = api_dispatcher.Dispatch(args, kwargs) 1077 if result is not NotImplemented: 1078 return result TypeError: Missing required positional argument . - concat을 사용할 때는 축을 반드시 지정해주어야 한다. 그렇지 않으면 위와 같이 오류가 뜬다. . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]])&gt; . - $2×1$ 벡터와 $2×1$ 벡터를 concat해서 $4×1$ 벡터로 만든다 . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]])&gt; . - $1×2$ 벡터와 $1×2$ 벡터를 concat해서 $2×2$ 벡터로 만든다 . a=tf.constant([1,2]) b=tf.constant([3,4]) a,b . (&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2])&gt;, &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4])&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4])&gt; . 의도하지 않은 결과가 나타나는 이유? : 전체 축에 대한 [ ]를 입력해주지 않았기 때문....... | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) a,b . (&lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 2]])&gt;, &lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[3, 4]])&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]])&gt; . - $1×2$ 벡터와 $1×2$ 벡터를 concat해서 $1×4$ 벡터로 만든다 . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]])&gt; . axis=0은 아래로 쌓이고 axis=1은 옆으로 쌓인다고 기억하자. | . - 3차원으로 확장해보자 : 첫 번째 축이 무엇인지 파악하는 것이 중요함 ! 다음 예제에서 첫 번째 축은 $2×3$행렬이 두 개 있는 3차원 축을 의미함 . a=tf.reshape(tf.constant(range(12)),(2,2,3)) b=-a a,b . (&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]])&gt;, &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]])&gt;) . - $2×2×3$ 벡터와 $2×2×3$ 벡터를 concat해서 $4×2×3$ 벡터로 만든다 . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]])&gt; . 합치고자 하는 것이 첫 번째 축이기 때문에 axis=0을 이용한다 | . - $2×2×3$ 벡터와 $2×2×3$ 벡터를 concat해서 $2×4×3$ 벡터로 만든다 . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5], [ 0, -1, -2], [ -3, -4, -5]], [[ 6, 7, 8], [ 9, 10, 11], [ -6, -7, -8], [ -9, -10, -11]]])&gt; . 합치고자 하는 것이 두 번째 축이기 때문에 axis=1을 이용한다 | . - $2×2×3$ 벡터와 $2×2×3$ 벡터를 concat해서 $2×2×6$ 벡터로 만든다 . tf.concat([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 2, 6), dtype=int32, numpy= array([[[ 0, 1, 2, 0, -1, -2], [ 3, 4, 5, -3, -4, -5]], [[ 6, 7, 8, -6, -7, -8], [ 9, 10, 11, -9, -10, -11]]])&gt; . 합치고자 하는 것이 세 번째 축이기 때문에 axis=2를 이용한다. | . - 다른 가능한 방법 . tf.concat([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 2, 6), dtype=int32, numpy= array([[[ 0, 1, 2, 0, -1, -2], [ 3, 4, 5, -3, -4, -5]], [[ 6, 7, 8, -6, -7, -8], [ 9, 10, 11, -9, -10, -11]]])&gt; . 이는 axis=2와 같다. 이는 -1이 뒤에서 첫번째 숫자를 의미하고 2가 뒤에서 첫번째 숫자(마지막 숫자)이기 때문이다 | . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5], [ 0, -1, -2], [ -3, -4, -5]], [[ 6, 7, 8], [ 9, 10, 11], [ -6, -7, -8], [ -9, -10, -11]]])&gt; . tf.concat([a,b],axis=-2) . &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5], [ 0, -1, -2], [ -3, -4, -5]], [[ 6, 7, 8], [ 9, 10, 11], [ -6, -7, -8], [ -9, -10, -11]]])&gt; . 마찬가지로 위 두 코드는 같은 결과를 나타낸다. | . - $(4,)$와 $(4,)$ concat $(8,)$ . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4])&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4])&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1, 2, 3, 4, -1, -2, -3, -4])&gt; . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [191], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . axis=0 으로 하면 $2×4$벡터가 만들어져야 하는 거 아닌가? | 즉, axis=1로 해야 길이가 $8$인 벡터가 만들어지는 거 아닌가? | 그런데 심지어 axis=1은 오류까지 뜬다?! | . 이유: $(4,),(4,),(8,)$은 길이가 각각 4,4,8인 튜플(1차원)이기에 애초에 축이 1개만 존재했기 때문이다. 즉 축이 2개가 존재하지 않기에 axis=1은 오류가 나고 축이 하나만 존재(1차원 형태)하기에 axis=0만 가능하다. 따라서 $2×4$ 또는 $4×2$(2차원 형태)는 애초에 불가능하다. | . tf.concat은 dimension(차원)을 변환시킬 수 없다. | . 즉, one dimension $ to$ 결과 one, two dimension $ to$ 결과 two, three dimension $ to$ 결과 three . dimension(차원)을 변환시키려면 다음과 같이 tf.stack을 사용해야 한다. . tf.stack . - $(4)$ 벡터와 $(4)$ 벡터를 concat해서 $(4,2)$ 벡터로 만든다 . $(4, )$ stack $(4, )$ $ to$ $(4,2)$ : 두 번째 축을 비어있다고 인식 | . a=tf.stack([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4])&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4])&gt;) . tf.stack? . Signature: tf.stack(values, axis=0, name=&#39;stack&#39;) Docstring: Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor. See also `tf.concat`, `tf.tile`, `tf.repeat`. Packs the list of tensors in `values` into a tensor with rank one higher than each tensor in `values`, by packing them along the `axis` dimension. Given a list of length `N` of tensors of shape `(A, B, C)`; if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`. if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`. Etc. For example: &gt;&gt;&gt; x = tf.constant([1, 4]) &gt;&gt;&gt; y = tf.constant([2, 5]) &gt;&gt;&gt; z = tf.constant([3, 6]) &gt;&gt;&gt; tf.stack([x, y, z]) &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[1, 4], [2, 5], [3, 6]], dtype=int32)&gt; &gt;&gt;&gt; tf.stack([x, y, z], axis=1) &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [4, 5, 6]], dtype=int32)&gt; This is the opposite of unstack. The numpy equivalent is `np.stack` &gt;&gt;&gt; np.array_equal(np.stack([x, y, z]), tf.stack([x, y, z])) True Args: values: A list of `Tensor` objects with the same shape and type. axis: An `int`. The axis to stack along. Defaults to the first dimension. Negative values wrap around, so the valid range is `[-(R+1), R+1)`. name: A name for this operation (optional). Returns: output: A stacked `Tensor` with the same type as `values`. Raises: ValueError: If `axis` is out of the range [-(R+1), R+1). File: c: users kko anaconda3 envs ds2022 lib site-packages tensorflow python ops array_ops.py Type: function . tf.stack()은 default가 axis=0이다 | . tf.stack([a,b]) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, 3, 4], [-1, -2, -3, -4]])&gt; . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, 3, 4], [-1, -2, -3, -4]])&gt; . 따라서 위 두 코드가 같다 | . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]])&gt; . - $(4)$ 벡터와 $(4)$ 벡터를 concat해서 $(4,2)$ 벡터로 만든다 . $(4, )$ stack $(4, )$ $ to$ $(4,2)$ : 두 번째 축을 비어있다고 인식 | . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, 3, 4], [-1, -2, -3, -4]])&gt; . - (2,3,4,5) stack (2,3,4,5) $ to$ (2,2,3,4,5) : 첫 번째 축이 비어있다고 인식 . a=tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b=-a a,b . (&lt;tf.Tensor: shape=(2, 3, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[100, 101, 102, 103, 104], [105, 106, 107, 108, 109], [110, 111, 112, 113, 114], [115, 116, 117, 118, 119]]]])&gt;, &lt;tf.Tensor: shape=(2, 3, 4, 5), dtype=int32, numpy= array([[[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]])&gt;) . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]]], [[[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]])&gt; . - (2,3,4,5) stack (2,3,4,5) . 두 번째 축이 비어있다고 인식 | (2,?,3,4,5) stack (2,?,3,4,5) $ to$ (2,2,3,4,5) | . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]])&gt; . - (2,3,4,5) stack (2,3,4,5) . 마지막 축이 비어있다고 인식 | (2,3,4,5,?) stack (2,3,4,5,?) $ to$ (2,3,4,5,2) | . tf.stack([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy= array([[[[[ 0, 0], [ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], [[ 5, -5], [ 6, -6], [ 7, -7], [ 8, -8], [ 9, -9]], [[ 10, -10], [ 11, -11], [ 12, -12], [ 13, -13], [ 14, -14]], [[ 15, -15], [ 16, -16], [ 17, -17], [ 18, -18], [ 19, -19]]], [[[ 20, -20], [ 21, -21], [ 22, -22], [ 23, -23], [ 24, -24]], [[ 25, -25], [ 26, -26], [ 27, -27], [ 28, -28], [ 29, -29]], [[ 30, -30], [ 31, -31], [ 32, -32], [ 33, -33], [ 34, -34]], [[ 35, -35], [ 36, -36], [ 37, -37], [ 38, -38], [ 39, -39]]], [[[ 40, -40], [ 41, -41], [ 42, -42], [ 43, -43], [ 44, -44]], [[ 45, -45], [ 46, -46], [ 47, -47], [ 48, -48], [ 49, -49]], [[ 50, -50], [ 51, -51], [ 52, -52], [ 53, -53], [ 54, -54]], [[ 55, -55], [ 56, -56], [ 57, -57], [ 58, -58], [ 59, -59]]]], [[[[ 60, -60], [ 61, -61], [ 62, -62], [ 63, -63], [ 64, -64]], [[ 65, -65], [ 66, -66], [ 67, -67], [ 68, -68], [ 69, -69]], [[ 70, -70], [ 71, -71], [ 72, -72], [ 73, -73], [ 74, -74]], [[ 75, -75], [ 76, -76], [ 77, -77], [ 78, -78], [ 79, -79]]], [[[ 80, -80], [ 81, -81], [ 82, -82], [ 83, -83], [ 84, -84]], [[ 85, -85], [ 86, -86], [ 87, -87], [ 88, -88], [ 89, -89]], [[ 90, -90], [ 91, -91], [ 92, -92], [ 93, -93], [ 94, -94]], [[ 95, -95], [ 96, -96], [ 97, -97], [ 98, -98], [ 99, -99]]], [[[ 100, -100], [ 101, -101], [ 102, -102], [ 103, -103], [ 104, -104]], [[ 105, -105], [ 106, -106], [ 107, -107], [ 108, -108], [ 109, -109]], [[ 110, -110], [ 111, -111], [ 112, -112], [ 113, -113], [ 114, -114]], [[ 115, -115], [ 116, -116], [ 117, -117], [ 118, -118], [ 119, -119]]]]])&gt; . 3&#44060;&#51032; array&#44032; &#51080;&#51012; &#44221;&#50864; . - 예제1: (2,3,4), (2,3,4), (2,3,4) 1) (2,3,4), (2,3,4), (2,3,4) -&gt; (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=-a c=2*a . a, b, c 모두 3차원이고 나타내고자 하는 것도 3차원이니 차원이 늘어나는 것은 아니다 -&gt; tf.stack가 아닌 tf.concat을 사용 . tf.concat([a,b,c],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]])&gt; . 6×3×4가 되었음 | . - 2. (2,3,4), (2,3,4), (2,3,4) -&gt; (2,9,4) . tf.concat([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11], [ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23], [-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23], [ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]])&gt; . - 3. (2,3,4), (2,3,4), (2,3,4) -&gt; (2,3,12) . tf.concat([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy= array([[[ 0, 1, 2, 3, 0, -1, -2, -3, 0, 2, 4, 6], [ 4, 5, 6, 7, -4, -5, -6, -7, 8, 10, 12, 14], [ 8, 9, 10, 11, -8, -9, -10, -11, 16, 18, 20, 22]], [[ 12, 13, 14, 15, -12, -13, -14, -15, 24, 26, 28, 30], [ 16, 17, 18, 19, -16, -17, -18, -19, 32, 34, 36, 38], [ 20, 21, 22, 23, -20, -21, -22, -23, 40, 42, 44, 46]]])&gt; . - 예제2: (2,3,4), (2,3,4), (2,3,4) 1) (2,3,4), (2,3,4), (2,3,4) -&gt; (3,2,3,4) -&gt; axis=0 . a, b, c 모두 3차원이지만 나타내고자 하는 것은 4차원이니 차원이 늘어난다 -&gt; tf.stack 사용 . tf.stack([a,b,c],axis=0) . &lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]], [[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]], [[[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]])&gt; . 2) (2,3,4), (2,3,4), (2,3,4) -&gt; (2,3,3,4) -&gt; axis=1 . tf.stack([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]])&gt; . 3) (2,3,4), (2,3,4), (2,3,4) -&gt; (2,3,3,4) -&gt; axis=2 . tf.stack([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 0, -1, -2, -3], [ 0, 2, 4, 6]], [[ 4, 5, 6, 7], [ -4, -5, -6, -7], [ 8, 10, 12, 14]], [[ 8, 9, 10, 11], [ -8, -9, -10, -11], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [-12, -13, -14, -15], [ 24, 26, 28, 30]], [[ 16, 17, 18, 19], [-16, -17, -18, -19], [ 32, 34, 36, 38]], [[ 20, 21, 22, 23], [-20, -21, -22, -23], [ 40, 42, 44, 46]]]])&gt; . 4) (2,3,4), (2,3,4), (2,3,4) -&gt; (2,3,4,3) -&gt; axis=3 . tf.stack([a,b,c],axis=3) . &lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy= array([[[[ 0, 0, 0], [ 1, -1, 2], [ 2, -2, 4], [ 3, -3, 6]], [[ 4, -4, 8], [ 5, -5, 10], [ 6, -6, 12], [ 7, -7, 14]], [[ 8, -8, 16], [ 9, -9, 18], [ 10, -10, 20], [ 11, -11, 22]]], [[[ 12, -12, 24], [ 13, -13, 26], [ 14, -14, 28], [ 15, -15, 30]], [[ 16, -16, 32], [ 17, -17, 34], [ 18, -18, 36], [ 19, -19, 38]], [[ 20, -20, 40], [ 21, -21, 42], [ 22, -22, 44], [ 23, -23, 46]]]])&gt; . - 예제3: (2,3,4), (4,3,4) -&gt; (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4)) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[-24, -25, -26, -27], [-28, -29, -30, -31], [-32, -33, -34, -35]], [[-36, -37, -38, -39], [-40, -41, -42, -43], [-44, -45, -46, -47]]])&gt; . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [22], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . tf.concat([a,b],axis=2) . InvalidArgumentError Traceback (most recent call last) Input In [23], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=2) File ~ anaconda3 envs ds2022 lib site-packages tensorflow python util traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~ anaconda3 envs ds2022 lib site-packages tensorflow python framework ops.py:7186, in raise_from_not_ok_status(e, name) 7184 def raise_from_not_ok_status(e, name): 7185 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7186 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . 위 두 코드는 차원이 맞지 않아 오류가 난다. | .",
            "url": "https://ki5n2.github.io/charcoal/2022/03/18/DS.html",
            "relUrl": "/2022/03/18/DS.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "DS 1. 단순선형회귀",
            "content": ". Data Science . lenture: Data Science_1nd week of lectures. | lenture date: 2022-03-07 | lecturer: Guebin choi | study date: 2022-03-16 | author: Kione kim | . . 이번 학기동안 배울 것: DNN(심층신경망), CNN(합성곱신경망), GAN(적대적생성신경망)인데, DNN을 바로 이해하기 어렵다. . | 따라서 다음의 과정을 학습한 후 심층 신경망으로 넘어갈 예정: (선형대수학 $ to$) 회귀분석 $ to$ 로지스틱회귀분석 $ to$ 심층신경망 . | . &#49440;&#54805;&#54924;&#44480; . - 카페 예제 . 온도가 높아지면 아이스아메리카노의 판매량이 증가한다는 사실을 알게 되었다. | 일기예보를 통해, 온도 $ to$ 아이스아메리카노 판매량 예측을 하고 싶다. | . import matplotlib.pyplot as plt import tensorflow as tf import numpy as np . - 자료생성 . x=tf.constant([20.1,22.2,22.7,23.3,24.4,25.1,26.2,27.3,28.4,30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . x와 y의 관계는 다음과 같이 가정 $${ bf y} approx 10.2 + 2.2{ bf x}$$ . tf.random.set_seed(50000) epsilon = tf.random.normal([10]) y = 10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([53.68625 , 60.12511 , 58.93714 , 60.65312 , 64.45385 , 66.9807 , 67.960144, 70.73565 , 72.87779 , 77.54677 ], dtype=float32)&gt; . x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([53.68625 , 60.12511 , 58.93714 , 60.65312 , 64.45385 , 66.9807 , 67.960144, 70.73565 , 72.87779 , 77.54677 ], dtype=float32)&gt; . data = tf.transpose(tf.concat([[x],[y]],0)) data . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 53.68625 ], [22.2 , 60.12511 ], [22.7 , 58.93714 ], [23.3 , 60.65312 ], [24.4 , 64.45385 ], [25.1 , 66.9807 ], [26.2 , 67.960144], [27.3 , 70.73565 ], [28.4 , 72.87779 ], [30.4 , 77.54677 ]], dtype=float32)&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,10.2+2.2*x,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x1a1b1b0d340&gt;] . 파란색 점: 데이터, 주황색 점선: 법칙 | 위 그림을 보니 $x$와 $y$가 선형관계가 있는 것처럼 보인다. 즉 아래의 식을 만족하는 $ beta_0, beta_1$가 있을 것 같다. | $y_{i} approx beta_1 x_{i}+ beta_0$ | . data . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 53.68625 ], [22.2 , 60.12511 ], [22.7 , 58.93714 ], [23.3 , 60.65312 ], [24.4 , 64.45385 ], [25.1 , 66.9807 ], [26.2 , 67.960144], [27.3 , 70.73565 ], [28.4 , 72.87779 ], [30.4 , 77.54677 ]], dtype=float32)&gt; . - 점원 A는 $ beta_0=15, beta_1=2$일 것이라고 주장하였고 점원 B는 $ beta_0=15.5, beta_1=2$일 것이라고 주장하였다. . 점원 A: $( beta_0, beta_1)$ = $(15,2)$ | 점원 B: $( beta_0, beta_1)$ = $(15.5,2)$ | . &#51092;&#52264;&#51228;&#44273;&#54633; . - $y_{i} approx beta_0 + beta_1$을 최소로 하는 $( beta_0, beta_1)$을 찾아보자 . - 잔체제곱합을 통한 점원 A, 점원 B 추정치 비교 . 20.1*2 + 15, 53.68625 . (55.2, 53.68625) . 22.2*2 + 15, 60.12511 . (59.4, 60.12511) . 20.1*2 + 15.5, 53.68625 . (55.7, 53.68625) . 22.2*2 + 15.5, 60.12511 . (59.9, 60.12511) . $i=1$일 때 점원 A의 주장이 더 잘 맞고 $i=2$일 때 점원 B의 주장이 더 잘 맞는다. | . - for 문을 사용하여 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$를 계산하여 비교해보자 . sum1=0 for i in range(10): sum1=sum1+(y[i]-15-2*x[i])**2 . sum2=0 for i in range(10): sum2=sum2+(y[i]-15.5-2*x[i])**2 . sum1 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=15.268475&gt; . sum2 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=14.011955&gt; . 점원 B의 추정치의 잔차제곱합이 조금 더 작다 $ to$ 점원 B의 주장이 더 적합하다 | 이 과정을 반복하면 최적의 추정치를 계산해낼 수 있을 것 같다. | . - 그러나 현실적으로 구현하기 어렵다 . : 왜냐하면 잔차제곱합이 0이 되지 않는다면, 무엇이 최적의 추정치인지 알 수 없다. 잔차제곱합이 0이 아니라면 항상 더 적합한 추정치가 있을 수도 있기 때문 . - 수식을 활용하여 찾아볼 수 있다 . $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$를 최소화하는 $ beta_0, beta_1$을 찾으면 되는데, 이는 아래와 같이 $ beta_0, beta_1$으로 각각 편미분하여 연립하여 풀면 된다. . $ begin{cases} frac{ partial}{ partial beta_0} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 frac{ partial}{ partial beta_1} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 end{cases}$ . 위 연립방정식을 편미분하면 다음과 같이 된다. . $ begin{cases} sum_{i=1}^{10} -2(y_i - beta_0 - beta_1 x_i)=0 sum_{i=1}^{10} -2x_i(y_i - beta_0 - beta_1 x_i)=0 end{cases}$ . 이를 정리하면 . $$ hat{ beta}_0= bar{y}- hat{ beta}_1 bar{x}$$ . $$ hat{ beta}_1= frac{S_{xy}}{S_{xx}}= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sum_{i=1}^{n}(x_i- bar{x})^2}$$ . 최적의 추정치$( hat{ beta}_0, hat{ beta}_1)$를 구할 수 있고 이를 통해 추세선을 그려볼 수 있다 . - 추정치는 다음과 같이 구할 수 있다. . Sxx = sum((x-np.mean(x))**2) Sxx . &lt;tf.Tensor: shape=(), dtype=float32, numpy=87.84898&gt; . Sxy = sum((x-np.mean(x))*(y-np.mean(y))) Sxy . &lt;tf.Tensor: shape=(), dtype=float32, numpy=202.18872&gt; . beta_1_est = Sxy/Sxx beta_1_est . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.3015487&gt; . beta_0_est = np.mean(y) - beta_1_est*np.mean(x) beta_0_est . &lt;tf.Tensor: shape=(), dtype=float32, numpy=7.8339195&gt; . - 그림을 그려보자 . plt.plot(x,y,&#39;.&#39;) plt.plot(x,beta_0_est + beta_1_est*x,&#39;--&#39;) plt.plot(x,10.2+2.2*x,&#39;--&#39;) # 세상의 법칙 . [&lt;matplotlib.lines.Line2D at 0x1a1b1b5cee0&gt;] . - 샘플수가 커질수록 주황색 선은 점점 초록색 선과 유사해진다. . 이는 매우 좋은 접근법이지만, 확장성이 떨어진다는 치명적 단점이 있다 | . - 매트릭스를 통해 확장성을 개선할 수 있다 . &#47784;&#54805;&#51032; &#47588;&#53944;&#47533;&#49828;&#54868; . - 우리의 모형 . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . 이를 풀어서 쓰면 . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . 아래와 같다. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . 벡터와 매트릭스 형태로 표현하면 . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ . - 손실함수의 매트릭스화 . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . 이를 벡터로 표현하면, . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - 미분하는 과정의 매트릭스화 . loss를 최소화하는 ${ boldsymbol beta}$를 구해야하므로 loss를 ${ boldsymbol beta}$로 미분한식을 0이라고 놓고 풀면 된다. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X} = - 2{ bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . 따라서 $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$을 풀면 아래와 같다. . $ 2{ bf X}^ top { bf y} = 2{ bf X}^ top { bf X}{ boldsymbol beta}$ . $ { bf X}^ top { bf X}{ boldsymbol beta} = { bf X}^ top { bf y}$ . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - 적용 . x=tf.constant([20.1,22.2,22.7,23.3,24.4,25.1,26.2,27.3,28.4,30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . tf.random.set_seed(50000) epsilon = tf.random.normal([10]) y = 10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([53.68625 , 60.12511 , 58.93714 , 60.65312 , 64.45385 , 66.9807 , 67.960144, 70.73565 , 72.87779 , 77.54677 ], dtype=float32)&gt; . - 방법 1 . tf.concat([[[1]*10],[x]],0) . &lt;tf.Tensor: shape=(2, 10), dtype=int32, numpy= array([[ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [20, 22, 22, 23, 24, 25, 26, 27, 28, 30]])&gt; . X = tf.transpose(tf.concat([[[1]*10],[x]],0)) X . &lt;tf.Tensor: shape=(10, 2), dtype=int32, numpy= array([[ 1, 20], [ 1, 22], [ 1, 22], [ 1, 23], [ 1, 24], [ 1, 25], [ 1, 26], [ 1, 27], [ 1, 28], [ 1, 30]])&gt; . - 방법 2 . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . X = tf.concat([[[1.0]*10],[x]],0).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - 깨알문법 . 역행렬 tf.linalg.inv사용 2.행렬 간 곱 @ 사용 | tf.linalg.inv(X.T @ X) @X.T @y . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([7.834038 , 2.3015506], dtype=float32)&gt; . beta_0_est, beta_1_est . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=7.8339195&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.3015487&gt;) . 그런데 값이 다르다...? . | 이유는 텐서플로우가 효율적 계산을 위해 조금 대충 계산하기 때문 . | . - 텐서플로우 내에 내장되어 있는 텐서플로우용 넘파이를 이용하여 다시 계산해보자 . import tensorflow.experimental.numpy as tnp . x=tf.constant([20.1,22.2,22.7,23.3,24.4,25.1,26.2,27.3,28.4,30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . y = 10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([53.68625 , 60.12511 , 58.93714 , 60.65312 , 64.45385 , 66.9807 , 67.960144, 70.73565 , 72.87779 , 77.54677 ], dtype=float32)&gt; . - 공식을 이용한 풀이 . beta1_est = sum((x-np.mean(x))*(y-np.mean(y))) / sum((x-np.mean(x))**2) ## Sxy/Sxx beta0_est = np.mean(y) - beta1_est * np.mean(x) . beta0_est, beta1_est . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=7.8339195&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.3015487&gt;) . - 벡터를 이용한 풀이 . X = tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.10000038], [ 1. , 22.20000076], [ 1. , 22.70000076], [ 1. , 23.29999924], [ 1. , 24.39999962], [ 1. , 25.10000038], [ 1. , 26.20000076], [ 1. , 27.29999924], [ 1. , 28.39999962], [ 1. , 30.39999962]])&gt; . tf.linalg.inv(X.T @ X) @X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([7.83391429, 2.30154889])&gt; . 거의 유사한 값이 나온다 | . &#44208;&#47200; . 벡터를 이용하여 tf.linalg.inv(X.T @ X) @X.T @ y를 계산하면 바로 $ beta$값이 바로 나온다. .",
            "url": "https://ki5n2.github.io/charcoal/2022/03/16/DS.html",
            "relUrl": "/2022/03/16/DS.html",
            "date": " • Mar 16, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ki5n2.github.io/charcoal/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ki5n2.github.io/charcoal/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}